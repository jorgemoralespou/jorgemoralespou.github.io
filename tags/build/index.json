[{"content":"Following the topic in this series, we should talk about the following factor to consider, breaking down Kubernetes applications development workflow into different stages: source code authoring, configuration authoring, packaging and deployment.\nIn kubernetes world, an application (or any component) is structurally composed of 3 different things: the container image(s), the deployment definitions and the configuration applied at every different deployment, as the same application can be deployed as different instances or in different logical environments. As we saw before, this can be referenced as the Application Physical View.\nThe application source code is authored, depending on the language can then be compiled, and finally packaged along with the application dependencies. This is the container image.\nDefining a container image is not an easy task, as the biggest problem is that along with your application, you’re packaging the user space that your application will need to run. This is something that typical developers don’t know how to do well. Not only that, but this is also something that many operations team would not want developers to do.\nFor this reason, there’s a set of tools that abstract away the process of building your source code into a container image. A plain Dockerfile is the simplest form, but then there are more advanced tools, like buildpacks or s2i that will take this problem away from the developer.\nOnce we have this application image created, we need to define how it will be deployed on the platform. Kubernetes provides many different controllers to run containers in different ways, stateless, stateful, daemons and jobs.\nBut there’s many other things that applications need in order to be consumable in a platform like Kubernetes. Services, Ingresses, PersistenceVolumes,\u0026hellip; All of them have their own resource definitions that can be complex and long. To simplify how users would have to deal with all these multiple resource definitions a common set of tools have emerged that provides a means to define all these resources at once. Tools like Helm, Kustomize, Ytt, Cue, Tanka and many more are amongst the most used ones.\nSome of these tools will also take the responsibility of applying these definitions to a Kubernetes cluster, but we can find more specialized tools that can do the task very well, like Kapp.\nA final set of tools can help developers abstract some of these stages away. Why? Because most of us don\u0026rsquo;t want to deal with the physical view of the application on Kubernetes. We just want to create our application and work with some expert on the platform that will help us with all that boilerplate for us. There are some interesting tools in this space like skaffold, tilt, devspace and others.\nWe need to keep reminding ourselves that Kubernetes is not an easy tool for developers, and despite many of us knowing some of these tools, we would have preferred not to, but sadly there\u0026rsquo;s not yet one tool that can abstract Kubernetes away from developers good enough.\nIn the coming weeks, I\u0026rsquo;ll be posting evaluations and comments on some of these tools as I progress the idea of what an ideal world for developers could be. But before that, we need to wait for the final article on this series, coming soon.\n","description":"","id":0,"section":"posts","tags":["kubernetes","appdev","cloudnative","devexp"],"title":"Developing applications on Kubernetes - Application development stages","uri":"http://jorgemoral.es/posts/2020_03_16-develop_apps_in_k8s_and_not_die_trying-app_dev_stages/"},{"content":"Now that we have all our required concepts clear, it is time to consider how Kubernetes defines an application. Or to be more precise, the lack of a standard for “Kubernetes application”.\nKubernetes is a project that exists for more than 5 years now and in all this time, we (application developers) have seen that there\u0026rsquo;s no support for what we call application, but there are some efforts that are worth talking about.\nKubernetes labels The first idea that the Kubernetes community had was to standardize on a set of labels that would be set on every resource that are part of an application. This is a basic concept embedded in the core of Kubernetes, label selection.\nBut, the biggest drawback I see to this approach is that, as a note on the Kubernetes doc say:\n “Note: These are recommended labels. They make it easier to manage applications but aren’t required for any core tooling”\n And if they are only a recommendation, they can not be really taken seriously. When we look into any of the latest Helm Charts that are published by companies like Bitnami, we can find that these labels are set, but if we do look at others, we find that are missing. Also when we look at applications deployed by Operators, these labels are missing. This seems to be the norm rather than the exception. For this reason, using labels is just a helper.\nSIG-APPS Application Expanding on this approach, Kubernetes SIG-APPS created a CRD to define what an application is. Now that Kubernetes is extensible through the definition of Custom Resource Definitions (a.k.a CRD), we can define a new Kubernetes resource (object) that will define an application. This is basically what the Application CRD and Custom Controller meant to do. This CRD relies heavily on the application labels, which is consistent with the recommendation that the Kubernetes community has made, but the CRD doesn\u0026rsquo;t seem to have gained much traction, although it seems to be back to life with some recent commits.\nMy opinion about this approach is that it\u0026rsquo;s adding an additional resource definition to the already existing kubernetes resource definitions required (e.g. Deployment, Service, Ingress, PersistenceVolumeClaim) which really doesn\u0026rsquo;t provide any simplification for the application developer. It also treats applications in a generic way, which can be fine as long as anything that defines an application can be properly dealt with. When looked from an \u0026ldquo;ops\u0026rdquo; or \u0026ldquo;kubernetes ecosystem engineer\u0026rdquo; perspective, this CRD can provide some application management benefits, but definitely not for the application author and the application authoring process.\nWhat I just said, in my humble opinion is key:\n Who is defining an application definition? What\u0026rsquo;s the goal for an application definition?  As I described in the intro article of the series there\u0026rsquo;s different type of users for Kubernetes, and the solutions that we\u0026rsquo;re seeing seems fit for one type of users (or two) but not for the other, the application developer/application author.\nLet\u0026rsquo;s see what additional innitiatives do exist out there before jumping into conclusions.\nCOTS - Commercial Off-The-Shelf applications Some Commercial Off-The-Shelf software (and ISV provided software) uses a specific CRD to define their application. A dedicated Custom Resource for the specific application is quite handy from a consumer point of view (developer) because it defines a new resource type that the developer will most likely understand. These custom resources doesn’t help one understand what is the relation it has with all the constituent resources, but it gives us an easy way to create a specific applications. As an example, we can create a Couchbase cluster just by creating a new Kubernetes object of kind: CouchbaseCluster. The downside to this approach is that many vendors will create the definition of their application in different ways, but at least it\u0026rsquo;s better than nothing. Also, an administrator might have difficulties to track down all the resources the application comprises, but this depends on the quality of the application author. Obviously, this is a really difficult solution to any developer as it involves the creation of resources that need to be deployed into the cluster by administrators only, so not any user can use these.\nCNAB Docker, Microsoft and Pivotal proposed that the easiest way to have an application defined is to package everything together in what they defined as “Cloud Native Application Bundle” (CNAB in short). CNAB proposes to facilitate the bundling, installing and managing of container-native applications and their coupled services. A CNAB bundle is a set of all the applications and deployment descriptors, plus the possible configuration definitions packaged in one single “transportable” artifact, that happens to be an “OCI image”. What this means is that CNAB is a solution for application distribution rather than for application definition or authoring. Still, developers need to create all the resources that will define the physical view, plus they will need to code the logic of how the application will be deployed. The good thing is that users can now manage (install/configure/delete) applications through a dedicated CLI.\nOAM There is yet another new definition being created: “Open Application Model”. It is being defined mostly by Microsoft and Alibaba. This specification tries to separate concerns on application deployment responsibilities by defining 3 main actors, “app developer”, “app ops” and “infra ops”. The first two focus on how the application will be deployed and run while the third should focus on the platform, Kubernetes.\nThis definition is still very immature and is not backed by many Kubernetes contributors, so we will still need to wait to see if (and how) it progresses. The separation of concerns is something that IMHO makes sense but there\u0026rsquo;s many other things in the implementation that are still very vague.\nSummary In this article I\u0026rsquo;ve tried to summarize some of the ongoing efforts to define what is an application in Kubernetes. I haven\u0026rsquo;t tried to go deep in any of the alternatives as I will be doing that over time in followup articles, where I will be giving a more thorough opinion on them. What I tried to raise in this article is the fact that in Kubernetes there\u0026rsquo;s no concept of an application what makes the task of developing applications difficult. If one decides to favor one approach it might get into a rough spot when trying to make his/her application universal, as the options here described are not standard and one can not expect them to work everywhere (maybe except using labels).\nIn the next article of the series I will touch the application development stages and some of the existing tools in the ecosystem. Stay tuned!\n","description":"","id":1,"section":"posts","tags":["kubernetes","appdev","cloudnative","devexp"],"title":"Developing applications on Kubernetes - Application definition","uri":"http://jorgemoral.es/posts/2020_02_27-develop_apps_in_k8s_and_not_die_trying-app_def/"},{"content":"As discussed in the intro article I\u0026rsquo;m going to talk about Developing applications that will run on a Kubernetes platform. The first thing we need to do before going deeper into the topic is to go over some basic concepts so that everything I talk about later has the proper context.\nWhat is an application? If you ask the question of what is an application to different people you will get different answers, almost one different answer per respondant. So here, I\u0026rsquo;m going to try to define what I understand as an application.\nAn application can be one single process/binary deployed in isolation, that has all the functionality it requires built-in. We have seen this pattern in the monolithic designs.\nBut we have also seen that sometimes this monolith is decomposed into smaller monolithic components, but that still are considered as monoliths in the classical monolithic designs.\nSometimes they often use a storage solution. Would the database be considered part of an application? Would the rest of the application work without the database?\nSometimes there’s more than just one database. Sometimes we have multiple databases or external services, like a message queue. Would we consider now that this is part of an application?\nAnd what if we have a huge amount of composing services? This is what sometimes we call a microservices type of design. What would be the application in this case?\nAnd what if we also add to the picture functions? An application can have all of these constituent services.\nFor this reason, what I define as an application is:\n “any software component or group of components that is designed for the end user”\n And the key identifiers that prove that an application has been designed for the end user are:\n Complete and fully functional: The application needs to work and not require any additional component that is nnot bundled/identified as part of the application. Does have a version number representing it: I can identify the complete application by a version number, even if the constituent components also have a version number which can be different for any of it. Multiple instances with different configuration can exist: Multiple instances of the application can exist and every application will have it\u0026rsquo;s own configuration. Two instances with the same identification and configuration will, most likely, be the same application. Can be easily installed: The process required to install the application should be simple enough that any end user can easily go through it, and obviously, will need to install the complete application (and constituent components).  Logical view versus physical view When we’re creating applications, as developers, we often care about how the components of our application will interact with each other. This is the logical view of our application. On the other hand, operations people would focus more on how these components are deployed and running. This is the physical view. It is important for us to understand these two views as different people will care about each view more.\nThe logical view often affects the application source code as how components will communicate with each other, or how to obtain the credentials to access a specific component are codified at the application source code. This is basically why developers need to care about the logical view of an application.\nThe physical view will determine the k8s resource definitions that will be used, whether a secret will be materialized as an Environment variable or as a file. If the application will have multiple replicas, what containers will be used, what ports will be used to listen,\u0026hellip; This is basically why operators or appsOps need to care about the physical view of an application.\nk8s application physical view In Kubernetes, when we look into the physical view of an application, we see that it consists of a set of containers images, containing runnable code, there’s also a set of resource descriptors that describe every component of the application, and for each instance of our application there will be a set of environmental configuration that gives our application different characteristics at runtime, these could be from just the name of the instance to any possible configuration. In order to have an application we need to have these three things defined, and hopefully under control.\nAlthough not so relevant for the developers, as I mentioned previously, the physical view of an application as here described is still important, mostly because Kubernetes doesn\u0026rsquo;t provide application abstractions that can make developers forget about this. But this will be the topic of the next article in the series.\n","description":"","id":2,"section":"posts","tags":["kubernetes","appdev","cloudnative","devexp"],"title":"Developing applications on Kubernetes - Concepts and Definitions","uri":"http://jorgemoral.es/posts/2020_02_24-develop_apps_in_k8s_and_not_die_trying-definitions/"},{"content":"Not too long ago I read a blog and found this quote:\n “It\u0026rsquo;s almost become boring to say that Kubernetes has become boring”\n Maciej Szulik, the author, is a Kubernetes engineer working for Red Hat and SIG CLI lead amongst other things.\nI’ve seen this quote being said more times than I would think, and all of the times it’s being said by a Kubernetes ecosystem engineer.\nWhat do I think they mean when they say Kubernetes has become boring?\nTo better understand the context, I always find important to know who\u0026rsquo;s talking. For that, I try to fit the person into one of the 3 different users I see on the Kubernetes world.\n“Application developers” are the people that works at Banks, Retailers, Consulting firms, almost everywhere and that create applications for the company they work for that will run on the platform.\n\u0026quot;Operations\u0026quot; are the people that will make sure that the application is running and that it does under certain conditions/SLAs/SLOs. They are responsible for the application even if they don\u0026rsquo;t really neccesarily need to know anything about it. And yes, I fit SREs under this category as well.\n\u0026quot;Kubernetes ecosystem engineers\u0026quot; are the people that creates the platform and any tooling around the platform. From Kubernetes, Istio, KNative, Tekton to things like Helm, Krew, k9s, any software that develops anything with any sort of dependency on Kubernetes APIs I categorize it as Kubernetes ecosystem engineer.\nTo me, it\u0026rsquo;s very important to draw this distinction as each one of these categories, and possible subcategories, have different skills, expertise and obviously requirements. They do totally different things and they use the tools and the platform in a totally different way.\nKubernetes was designed to do one thing, “Production-Grade Container Orchestration”. After 4 years of the project, most of the basic required constructs are there and the goals of the project are met. It does container orchestration really well. That’s why, for someone like Maciej, a Kubernetes engineer, it is boring.\nBut obviously, not everyone feel the same.\nFor “Application developers”, Kubernetes is just where their applications will be deployed and will run. If their applications were to be deployed and run on Virtualization, probably their life would be the same.\nSo, what do “application developers” care about?\nThey do care about being productive creating, updating, maintaining, fixing and evolving business applications. And they do care about using tools that are easy to learn and understand.\nHence, I can confidently say that Kubernetes despite being boring, it’s also hard. And for “application developers” it is “really, really hard”.\nThis one is the first article of a series where I will describe some of my thoughts around “developing applications for Kubernetes” and will continue presenting some of the tools I find useful.\nTo ilustrate my thoughts, I will use what I call the \u0026quot;Kubernetes application development triangle\u0026quot;.\nBut if you want to know more about this, you\u0026rsquo;ll have to wait for the next article.\n","description":"","id":3,"section":"posts","tags":["kubernetes","appdev","cloudnative","devexp"],"title":"Developing applications on Kubernetes - Intro","uri":"http://jorgemoral.es/posts/2020_02_21-develop_apps_in_k8s_and_not_die_trying-intro/"},{"content":"My name is Jorge Morales, and I work for VMware as Cloud Native Developer Advocate.\nYou can probably not guess that I\u0026rsquo;m spanish from my accent, but probably from how I write you\u0026rsquo;ve already realized that either I\u0026rsquo;m not native speaker/writer or I failed on grammar at school. Well, it\u0026rsquo;s the first option ;-)\nI\u0026rsquo;ve been on the IT industry for almost 20 years, and in all these years I\u0026rsquo;ve learnt a lot of things, but the most important is that you need to stimulate knowledge sharing, as that\u0026rsquo;s how probably all of us have learnt more than 80% of what we know, by reading other\u0026rsquo;s blogs, documents and opinions.\nSo this is basically what I do here. Share with you some of my thoughts, some of my knowledge and some of my experience.\nI hope you like it, and of course, you\u0026rsquo;re free to read me or not, but if you do, please let me know what you\u0026rsquo;re interested in, from what I know, and I\u0026rsquo;ll try to make your life easier. That\u0026rsquo;s one of the things that makes me happy. And I like being happy.\nThere\u0026rsquo;s some links to places where you can get to know me more on the left, so click only if you\u0026rsquo;re really interested. Else, spend you\u0026rsquo;re time in something more useful.\nLife\u0026rsquo;s too short to waste it with boring things ;-)\nCheer up!!! It\u0026rsquo;s another great day.\n","description":"","id":5,"section":"","tags":null,"title":"About me","uri":"http://jorgemoral.es/about/"},{"content":"KubeCon China 2019 - Shangai\n","description":"","id":7,"section":"talks","tags":["kubecon"],"title":"KubeCon China 19","uri":"http://jorgemoral.es/talks/2019-06-25-kubecon-china-19/"},{"content":"Have you ever developed applications on a platform like Red Hat OpenShift?\nI’m a Java developer with more than 15 years of coding experience, and while I’ve been working with OpenShift for over three years now, I never found it easy to use or compelling as a day to day development platform. Why? There are many reasons to this question, but the key ones are, complexity and speed. Before you call me a troll, allow me to explain.\nAs a Java developer, one of the things that I’ve done for a long time has been to build using maven (I never got to gradle myself). Maven has provided me the ability to do almost anything I wanted with a simple command line tool. Almost anything meant that I typically had my runtimes installed and available on my local machine, and maven only had to deploy (or copy) the generated artifact into the appropriate location. Sometimes this even triggered a restart of my runtime, or just a reload if the integration with the runtime allowed for such fanciness.\nIt took me a long time to get used to maven. I bet that it was the same for most of you. But once I knew (more or less) how to use it, it was pretty cool. I could share my source code, with a pom.xml file to any of my colleagues and they could build and test the application, just the same way I did.\nThe bigger challenge back then was not creating the application, but ensuring that my fellow developers were running it the same way. The issue was that many of us installed our runtimes and databases locally, on heterogeneous hardware, probably with different operating systems, or at least different versions of the same OS.\nThe solution was to standardize our runtime environments. I was a fervent user of Vagrant. I created development environments using Vagrant, so all of us developers could use the same runtimes, with the same configuration and the same application that maven built.\nFast-forwarding some years: containers took the place of these standardized runtime environments, and Kubernetes environments took the place of orchestrating all the pieces required for an application to run. You could now use Kubernetes locally, on your laptop, using minikube or Minishift. But now, we needed to create containers rather than applications, as these are the new artifacts. This posed a new challenge to me, and probably to many of you.\nBuilding containers with your application in it can be a simple or complex process. There are a variety of tools that makes the process simpler. Tools that take your source code, build it and incorporate the generated artifact into a container that already had the desired runtime your application would use.\nOpenShift s2i is one of these mechanisms. Heroku and Cloud Foundry Buildpacks is another approach. As a Java developer, I feel that any of these opinionated approaches works much better than dealing with a Dockerfile yourself.\nNow, the platform you use to run your application can also build your application, as long as it can access the source code for it. But the process is slower than what you’re used to. And while slower can work sometimes –typically those times where you’re not waiting for it to complete– there are other times when you need the build and deployment to finish promptly to start your validation and testing, or just to continue your development work.\nHow can we make this process as fast as possible but also have your applications running on these wonderful environments (OpenShift or Kubernetes)?\nWe’ve been pondering this problem ourselves for a long time. We need a fast inner development loop, that is, code, build, deploy, test, code, build, deploy, test, again and again. We don’t want to push our code to a git server for the platform to build it, as this is a longer process. Also, should I commit code to my git server if I’m not sure whether it works as it should?\nWith this premise in mind, we are building a command line tool that we call odo, which stands for OpenShift Do.\nThe logic behind this tool is simple. We deploy onto the platform specialized containers that know how to build an application, but that also have the runtime for the application included and already running. We then only need to push the application to this container. For this, we have the option of pushing the source code and the container will create the artifact for us, or we can just push the artifact if we have already built it locally on our local machine. The tool used to push the application will then instruct the container to reload the runtime with the new application in it. This will only reload the runtime and not the whole container/pod, making the process as fast as it would be if I host that runtime locally on my machine.\nThis is a development pod, which knows how to build and/or run my application. What is convenient though, is that we can use a regular s2i container –one that OpenShift uses– for the process. We don’t need any special or additional capabilities.\nAs an example is worth more than a thousand words:\nAs you would have probably seen in the previous example, the process we went through is recognizable by you, even if you’re not an OpenShift or Kubernetes expert. How is that possible?\nWe took the decision to make the tool as user (a.k.a. developer) friendly as we could, which means, that we are aware many users don’t fully understand all the complex constructs a platform like OpenShift or Kubernetes provides. We wanted to make it simple, and immediately familiar. So we decided the language the tool was going to provide should be natural to developers.\nAnother important observation we embodied in the tool is that developers usually work on a single component (part of an application) at a time. So, while a developer can have a quite complex application (consisting of multiple deployed components or services), he or she is only going to work through the inner development loop of one of these components at a time. That means our CLI was built with the concept of a component at its center.\nTo create a component corresponding to the type of runtime of your application, you will select from one of the available component types in the catalog. Once you have the component created, you only need to push your code (or pre-built artifact) to it, the tool will do the rest. If you want to access your component, you create a URL. If you want to provide persistence for your component, you create storage for the component. If you want to configure your application, you create a config for your component.\nAll of these constructs have been designed with a developer experience in mind, and should be straightforward to use for most (if not all) developers, regardless of the programming language. They no longer need to know the specifics of the platform where their application will be deployed and running.\nThis is just an introduction to a project we’re developing. It’s currently under heavy development and only community supported. It’s already in a working state, although not complete as yet. We want to hear your feedback. If you want to try it, head to the project’s page and read the installation instructions. If you have feedback, please, open an issue on the project’s GitHub repository.\nIn the coming weeks, we’ll be talking much more about this project. The current state, how specific features work, and what we have in mind for the future. Stay tuned!!!\nDevelop in your own style, forget about the platform!\n","description":"","id":12,"section":"posts","tags":["openshift","development","local","devexp","odo"],"title":"Developing applications on OpenShift in an easier way","uri":"http://jorgemoral.es/posts/2018-12-26-developing-apps-in-openshift-with-odo/"},{"content":"OpenShift admins choose different architectures for their installations, but many use two discrete clusters to physically divide development and testing workloads from production deployments.\nWe recommend having some Continuous Integration (CI) process in nearly every development scenario, to orchestrate the lifecycle of applications from the initial commit all the way into production. Continuous Integration can imply many different tasks related to application development, testing, and releases. In this article, I won’t directly address whether one should do Continuous Delivery or Continuous Deployment, as that depends on the maturity and complexity of the organization and the application.\nNo matter which technique you apply in your CI process, the end goal is for applications to move from development into production. In the world of containers running on orchestrated clusters, applications are packaged as container images and stored in registries. So promoting an app from development to production often means copying a container image from one registry to another. The OCI Image Spec gives us a standard format for container images, making such OCI container images portable between container registries and runtimes.\nAt Red Hat, we’ve been working for some time on tools for handling OCI images. The OCI specification means that an image can be created, stored, or executed with any tool that adheres to the spec. Whether you want to build an image with Buildah, or run it on a Kubernetes/OpenShift cluster using CRI-O, or keep it old school and do both jobs with Docker, the image spec means the choice is up to you. OCI-standard container images can be shared among all of those tools.\nWhen it comes to inspecting and transporting images, a tool called skopeo proves very useful. Skopeo inspects container images in any of the places where an OCI image can be stored. It can also copy container images from one location to another. If you want to copy an image from your laptop’s local docker storage to the local CRI-O container store, it’s as easy as:\n1  skopeo copy docker-daemon:myregistry/myimage:1.0.0 container-storage:myregistry/myimage:1.0.0   Skopeo is also useful for copying images between two remote docker registries, such as the registries of two different OpenShift clusters.\n1  skopeo copy docker://clusterA:5000/myregistry/myimage:1.0.0 docker://clusterB:5000:myregistry/myimage:1.0.0   What’s so revolutionary about copying container images around? Well, the trick is that skopeo is a standalone tool that doesn’t require a docker –or any other– daemon, so it can easily be used in Continuous Integration pipelines.\nHere’s an example of a CI pipeline:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77  def namespace, appReleaseTag, webReleaseTag, prodCluster, prodProject, prodToken pipeline { agent { label \u0026#39;skopeo\u0026#39; } stages { stage(\u0026#39;Choose Release Version\u0026#39;) { steps { script { openshift.withCluster() { // Login to the production cluster  namespace = openshift.project() prodCluster = env.PROD_MASTER.replace(\u0026#34;https://\u0026#34;,\u0026#34;insecure://\u0026#34;) withCredentials([usernamePassword(credentialsId: \u0026#34;${namespace}-prod-credentials\u0026#34;, usernameVariable: \u0026#34;PROD_USER\u0026#34;, passwordVariable: \u0026#34;PROD_TOKEN\u0026#34;)]) { prodToken = env.PROD_TOKEN } // Get list of tags in the ImageStream to show the release-manager  def appTags = openshift.selector(\u0026#34;istag\u0026#34;).objects().collect { it.metadata.name }.findAll { it.startsWith \u0026#39;app:\u0026#39; }.collect { it.replaceAll(/app:(.*)/, \u0026#34;\\$1\u0026#34;) }.sort() timeout(5) { def inputs = input( ok: \u0026#34;Deploy\u0026#34;, message: \u0026#34;Enter release version to promote to PROD\u0026#34;, parameters: [ string(defaultValue: \u0026#34;prod\u0026#34;, description: \u0026#39;Name of the PROD project to create\u0026#39;, name: \u0026#39;PROD Project Name\u0026#39;), choice(choices: appTags.join(\u0026#39;\\n\u0026#39;), description: \u0026#39;\u0026#39;, name: \u0026#39;Application Release Version\u0026#39;), ] ) appReleaseTag = inputs[\u0026#39;Application Release Version\u0026#39;] prodProject = inputs[\u0026#39;PROD Project Name\u0026#39;] } } } } } stage(\u0026#39;Create PROD\u0026#39;) { steps { script { openshift.withCluster(prodCluster, prodToken) { openshift.newProject(prodProject, \u0026#34;--display-name=\u0026#39;CoolStore PROD\u0026#39;\u0026#34;) } } } } stage(\u0026#39;Promote Images to PROD\u0026#39;) { steps { script { openshift.withCluster() { def srcApplicationRef = openshift.selector(\u0026#34;istag\u0026#34;, \u0026#34;app:${appReleaseTag}\u0026#34;).object().image.dockerImageReference def destApplicationRef = \u0026#34;${env.PROD_REGISTRY}/${prodProject}/app:${appReleaseTag}\u0026#34; def srcToken = readFile \u0026#34;/run/secrets/kubernetes.io/serviceaccount/token\u0026#34; sh \u0026#34;skopeo copy docker://${srcApplicationRef} docker://${destApplicationRef} --src-creds openshift:${srcToken} --dest-creds openshift:${prodToken}\u0026#34; } } } } stage(\u0026#39;Deploy to PROD\u0026#39;) { steps { script { openshift.withCluster(prodCluster, prodToken) { openshift.withProject(prodProject) { def template = \u0026#39;https://raw.githubusercontent.com/openshift-labs/myapp/myapp-template.yaml\u0026#39; openshift.apply( openshift.process(\u0026#34;-f\u0026#34;, template, \u0026#34;-p\u0026#34;, \u0026#34;APPLICATION_IMAGE_VERSION=${appReleaseTag}\u0026#34;, \u0026#34;-p\u0026#34;, \u0026#34;IMAGE_NAMESPACE=\u0026#34;) ) } } } } } } }   In our OpenShift workshops, we use this pipeline to illustrate Continuous Delivery onto the platform. I could go on about Continuous Delivery best practices, but today’s post focuses on skopeo, the key to how this pipeline copies container images between two clusters’ registries.\nWith skopeo, it’s also easy to provide authentication credentials for secured registries. There are different mechanisms to supply your credentials, and all of them are easy to embed in a pipeline or to use directly from the command line.\nIf you need to move container images between public registries or to promote images from a dev registry into prod, try out skopeo. Skopeo is a stable tool with a track record of extensive use at Red Hat over the last year, but if you run into problems, you can report them directly to the developers at the project’s GitHub repository.\nIn a world where container images are compliant with the OCI Image Specification, skopeo is a daemonless tool that helps you copy images between different storage locations. Whether this storage is local to a node, like docker-storage and CRI-O storage, or it’s remote, such as in a copy from one Docker registry to another, skopeo is the tool for the job.\n","description":"","id":14,"section":"posts","tags":["openshift","skopeo","image","promotion"],"title":"Promoting container images between registries with skopeo","uri":"http://jorgemoral.es/posts/2018-08-03-promoting-images-with-skopeo/"},{"content":"If you’ve heard of minishift, the OpenShift environment for your laptop, or if you’re using the Container Development Kit (CDK) at work, you’re probably building applications on the OpenShift Container Platform. This post is for you. If you’re new to OpenShift and these names aren’t familiar yet, check out the this other minishift blog first to get the most value from the content below.\nUpgrades: The Wisdom of Impermanence Developers often ask how minishift installations should be upgraded. This question is especially relevant with minishift v1.19.0, because the operating system running in minishift’s virtual machine changed from Boot2Docker to CentOS. Devs usually worry about upgrading a minishift instance where they have their working application code deployed.\nThe first answer to that worry is very simple: Don’t upgrade the virtual machine. It’s possible to upgrade just the minishift binary to the latest version, without starting a new instance or upgrading existing ones. Existing, older virtual machines will provide a perfectly suitable kernel environment for a long enough time to simply not worry about it in the short term.\nMinishift Instances as Ephemeral Workspaces The second answer is more of a philosophy that helps me develop applications that are easy to test and move into production in any OpenShift environment, and it’s what lead to this article about minishift environment care. Treat minishift instances as ephemeral workspaces. Don’t expect them to run forever. Instead, concentrate on making your work easy to deploy on any instance. This not only uses your laptop’s resources more efficiently, but also helps foster good development practices.\nI’m a developer advocate, which means that I develop applications on OpenShift to show other developers how to take advantage of OpenShift features and adjust application architectures to get the most out of the platform. I need to create demos that will execute repeatedly at many different times, and, most importantly, that are meant to run by others as well. This means even demo apps need to be ready to move between environments so they will run both “in development” on minishift on my own laptop, and “in production” on the laptops of curious programmers.\nI usually find myself working on different projects and applications at different times throughout the day. Making such multitasking easier leads to a suggestion that makes our “ephemeral” philosophy a real-world practice.\nHave a separate minishift profile for each application you’re working on. Minishift provides a concept called profiles. A profile is a named minishift instance that can be created, configured, started, used, stopped, and deleted, independent of those actions being taken on any other profile.\nHow do you create a profile? It’s as easy as naming the profile you want to use. If the profile already exists, it will become the active profile, and minishift commands will be executed in it. If a profile of that name doesn’t exist, it will be created and made active.\n1  minishift profile set careandfeeding   This command creates a directory named for the profile beneath your minishift profiles directory, in this case $HOME/.minishift/profiles/careandfeeding. The profile’s VM disk file and configuration are stored here.\nYou can list the known profiles:\n1  minishift profile list   Now that you know what a profile is, I recommend creating a profile for every application or project you work on.\nHere is an example workflow, showing how to switch between two profiles for two different applications, appA and appB:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  # Create profile for application A $ minishift profile set appA # Start working on development of application A $ minishift start \u0026lt;code\u0026gt; # oc new-app . --name appA\u0026lt;/code\u0026gt; # Finish work on application A $ minishift stop # Let’s work on application B $ minishift profile set appB $ minishift start \u0026lt;code\u0026gt; # oc new-app . --name appB\u0026lt;/code\u0026gt; # Finish work on application B $ minishift stop   This workflow can be repeated as many times as needed. You might keep working on an application for a long time, which means that your daily workflow will be just starting minishift, working on your application, and then stopping minishift at the end of the day. The following day you can repeat this shortened workflow, until it’s time to work on another app. Then, you just need to activate that app’s profile and start its instance. While you can have more than one profile’s VM running at the same time, I don’t recommend it. It uses more computing resources on your laptop, and you can lose track of which profile is active and currently receiving your minishift commands. My habit is to run only the active profile and to stop minishift before switching to another profile and doing a minishift start there.\nThere are many advantages to this way of working, but the most important is that the amount of memory and CPU that your minishift instance will require will be kept to a minimum, which can be critical for many developers constrained by the limits of today’s laptop computers.\nWhat are the drawbacks? In my opinion, the biggest drawback to the practice of multiple profiles is that each profile will use disk space. If you have many profiles, they’ll eventually consume a lot of disk space. This brings me to another recommendation.\nDelete long-inactive VMs – but not their Profiles Notice we are talking about the actual VM disk image, and not the profile that contains it. A profile is more than just the VM. A profile maintains an instance’s configuration. You want to keep that.\n1 2 3  # Use a profile to work as shown before # When this app is complete or otherwise inactive for a long time $ minishift delete   What kind of madness is this? We know the profile maintains the instance’s configuration, but deleting the VM usually means that the next time you need to work in this profile, minishift must bootstrap OpenShift all over again, pulling megabytes of OpenShift images from the internet. This leads to my third recommendation.\nUse minishift image caching to save container images on the host Minishift provides a handy feature for caching container images on a local filesystem. With image caching correctly configured, you will not need to download images again when a profile’s VM is recreated. Instead, images are copied from the filesystem cache to the active profile.\nImage caching requires some configuration. We need to tell minishift which images it should cache. You can do that beforehand, when you know all the images needed for work in the profile, or after you have bootstrapped the VM in a profile for the first time.\nLet’s walk through the process.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  # Create profile for application A $ minishift profile set appA # Let’s activate image caching functionality for this profile $ minishift config set image-caching true # If we know we need to persist some images, let’s configure them before hand $ minishift image cache-config add # Start minishift and work $ minishift start # As we work, we might want to cache some other images. # We can list the images contained in the VM $ minishift images list --vm MYIMAGE3 # And those stored locally in the cache $ minishift images list # Let’s cache another image added after starting the profile’s VM af $ minishift image cache-config add # cache-config view will list the $ minishift image cache-config view # Once its project is complete, we can garbage-collect the profile’s VM $ minishift delete # But with a new feature idea, we want to work on appA again $ minishift profile set appA $ minishift start # This will fetch the profile’s images from the local cache without downloading them again.   Minishift addons help manage additional dependencies So far things are great. My VM is running, and it has everything I need to work on the next great feature for appA. Except for one additional dependency! Managing ancillary requirements and configurations like this is exactly what minishift addons are designed to do.\nA minishift addon is a script that extends the default minishift start behavior, so that we can provide additional bootstrap steps, deploy arbitrary software (like appA’s additional dependency!), or adjust the configuration of the OpenShift instance.\nAdd-ons can be shared between multiple users and multiple profiles, which makes them even more useful. A set of default addons are provided with minishift, and there is a repository of community contributed addons, too.\nThere are add-ons I consider essential and add to nearly every profile I create. Beyond those basics, I like to script any bootstrapping an application requires as an addon as well so I can manage it alongside the app’s source code in a version control system.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  # Create a profile for appA $ minishift profile set appA # The admin-user addon goes into nearly every profile I create $ minishift addon enable admin-user # List the profile’s addons $ minishift addon list Default add-ons \u0026#39;anyuid, admin-user, xpaas, registry-route\u0026#39; installed - admin-user : enabled P(0) - anyuid : disabled P(0) - registry-route : disabled P(0) - xpaas : disabled P(0) # Install the addon for bootstrapping appA’s dependencies. This copies the addon to the profile folder and enables it. $ minishift addon install addon/appA-deps --enable # List the profile’s addons again $ minishift addon list Default add-ons \u0026#39;anyuid, admin-user, xpaas, registry-route\u0026#39; installed - admin-user : enabled P(0) - anyuid : disabled P(0) - registry-route : disabled P(0) - xpaas : disabled P(0) - appA-deps : enable P(0)   Now, every time I recreate the VM in this profile, these add-ons will be active, bootstrapping the environment I need. I have to create my application, everything else is ready to go when I start minishift in the appA profile.\nWhat is great about this workflow is that the source control and add-ons for the app’s dependencies are stored in the same version control repository with its source code, making it easy to manage changes over time, and making the app repo a one-stop-shop for a future developer to get everything needed to get right to work.\nUsing profiles helps me keep my minishift environments up to date and my application code ready for deployment in any environment. Combining them with minishift addon scripts and the other practices I’ve outlined here makes recreating a set of dependencies for each instance easy, too. There are a few more things that I’d like to automate or manage more efficiently. While minishift doesn’t fully support this short list of improvements yet, you can see in the linked GitHub issues that work on them is in progress:\n Enable managing minishift configurations as source code, in a version control system, making it easier to replicate, manage, and share profiles (#2531)\nCreate a profile from a template (#1517) and #1649)\nImprove cache speed and reusability of layers/images (#2532)  Minishift helps me be more productive, and I try to model not just demo apps, but also processes, that mirror and advance how front-line application programmers work in the real world. With any luck, maybe you’ll find some of the tips from my experience useful. Drop me a like, comment or idea @jorgemoralespou on Twitter.\n","description":"","id":15,"section":"posts","tags":["openshift","development","local","devexp","minishift"],"title":"Care and Feeding of Minishift Development Environments","uri":"http://jorgemoral.es/posts/2018-07-12-care-and-feed-minishift-dev-envs/"},{"content":"For some time I\u0026rsquo;ve been hearing about Helm and have been asked by people how they could deploy into OpenShift applications defined as Charts, the format Helm uses to package an application.\nOne of the really nice features that minishift \u0026gt;= 1.2.0 introduced was the concept of an addon which is a way to provide additional capabilities to your minishift local environment. As this feature is really interesting, and evolving really nicely, I have developed some addons that allow me to extend my minishift capabilities by issuing a single command.\nIn this blog I will describe how to deploy helm into minishift OpenShift, and then I will deploy a sample application using a helm chart.\nNote that this is not supported and it is used for the solely purpose of supporting and describing the work that has been done around minishift addons. If you want to use what here is described, it’s at your own risk.\nHelm in a Handbasket I have taken this part from the helm documentation, as it perfectly introduces helm in a few sentences:\nInstall You will definitely need to install helm on your laptop, as it consists of two parts, a client(helm) and a server(tiller). To find the latest client go here and find the binary that suits your Operating System.\nUnpack the helm binary and add it to your PATH and you are good to go!\nThe server part, tiller, will be installed in minishift via an addon.\nStart minishift (Use virtualbox) There is already a guide on how to install minishift, so I will expect that you have followed it and that you have minishift already working.\nI will also expect that you\u0026rsquo;re using the latest minishift version available today (1.3.0) or a newer one, if there is one already available as you read this.\nPresuming that you read this blog as soon as it is published, and that you don\u0026rsquo;t have a minishift instance already up and running, this is the process you would follow to be able to continue with the blog.\nFirst, I will install the default addons that come shipped with minishift, and then I will enable an addon that will create an admin user, so I can easily log into the minishift OpenShift web UI as admin of the platform.\n1 2  minishift addons install --defaults minishift addons enable admin-user   This process instructs every minishift instance that will be created from this point to install this addon, so it\u0026rsquo;s a one time step.\nNow, I will create my minishift instance. I\u0026rsquo;m using the latest available openshift version as the time of writing, but you could just be using the default shipped with minishift. Also, I\u0026rsquo;m using virtualbox as virtualization technology, but you could again be using the one you prefer from all the available technologies for your Operating System. And also, I like to give the VM enough cpu and memory so that I can comfortably work.\n$ minishift start --vm-driver=virtualbox --openshift-version=v3.6.0-rc.0 --cpus=2 --memory=8192 Starting local OpenShift cluster using 'virtualbox' hypervisor... Downloading ISO 'https://github.com/minishift/minishift-b2d-iso/releases/download/v1.0.2/minishift-b2d.iso' 40.00 MiB / 40.00 MiB [===================================================================================================================================================================================================] 100.00% 0s Downloading OpenShift binary 'oc' version 'v3.6.0-rc.0' 33.74 MiB / 33.74 MiB [===================================================================================================================================================================================================] 100.00% 0s Starting OpenShift using openshift/origin:v3.6.0-rc.0 ... Pulling image openshift/origin:v3.6.0-rc.0 Pulled 1/4 layers, 26% complete Pulled 2/4 layers, 64% complete Pulled 3/4 layers, 77% complete Pulled 4/4 layers, 100% complete Extracting Image pull complete OpenShift server started. The server is accessible via web console at: https://192.168.99.100:8443 You are logged in as: User: developer Password: [any value] To login as administrator: oc login -u system:admin -- Applying addon 'admin-user':.. Install helm addon (tiller - server side) Now that we have minishift up and running, we can install helm\u0026rsquo;s server part, tiller. For this, I have created an addon that simplifies the installation.\nThe process is as simple as install my addon and the apply the addon, so that helm tiller will be provisioned one time on this machine. Note that I use apply instead of enable, as I just want this install to happen for the current minishift instance and not every time I create a new minishift instance.\n$ cd /tmp $ git clone https://github.com/jorgemoralespou/minishift-addons $ cd minishift-addons $ minishift addons install helm $ minishift addons apply helm -- Applying addon 'helm':...... Get Tiller host URL by runninr these commands in the shell: export TILLER_HOST=\u0026quot;192.168.99.100:$(oc get svc/tiller -o jsonpath='{.spec.ports[0].nodePort}' -n kube-system --as=system:admin)\u0026quot; Initialize the helm client, if not done already e.g. helm init -c Search for an application: e.g. helm search And now deploy an application e.g. helm --host $TILLER_HOST --kube-context default/192-168-99-100:8443/system:admin Now that we have installed tiller, we can log into the minishift OpenShift web UI as admin. Remember we have enabled the admin-user addon, so that there is an admin user with admin password to log in the web UI.\nThis will open the web UI in our browser.\nminishift console And once we log in with the admin credentials:\nWe will be able to see tiller deployed in the kube-system namespace.\nAs you would probably have noticed, it\u0026rsquo;s the \u0026ldquo;#2\u0026rdquo; deployment. This is mostly because the original helm deployment has been altered to use a dedicated serviceaccount helm, that will be given the required permissions cluster-admin. As I like to do, I tried to minimize who will get this escalated permissions to just the serviceaccount tiller will use.\nNOTE: Helm currently has a shortcoming when it comes to work nicely in multitenant environments. Tiller requires cluster-admin role to properly work, and it’s not possible to install in an unprivileged manner in your own project/namespace to provide you with the ability to deploy applications there.This is in order to make the deployment as secure as possible.\nAlso, tiller is exposed through a routenodePort that we will use later. We create an environment variable to refer to tiller.\n$ export TILLER_HOST=\u0026quot;$(minishift ip):$(oc get svc/tiller -o jsonpath='{.spec.ports[0].nodePort}' -n kube-system --as=system:admin)\u0026quot; $ echo $TILLER_HOST 192.168.99.100:30609 Install helm (client side) It is time to configure our client helm to use tiller in minishift. I presume you have already installed the helm binary and added to the path, so you can use helm client.\nTo verify it:\n$ helm version Client: \u0026amp;version.Version{SemVer:\u0026quot;v2.5.0\u0026quot;, GitCommit:\u0026quot;012cb0ac1a1b2f888144ef5a67b8dab6c2d45be6\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;} Error: cannot connect to Tiller Obviously it can not connect to tiller. So let\u0026rsquo;s configure our helm client instance:\n$ helm init -c Creating /Users/jmorales/.helm Creating /Users/jmorales/.helm/repository Creating /Users/jmorales/.helm/repository/cache Creating /Users/jmorales/.helm/repository/local Creating /Users/jmorales/.helm/plugins Creating /Users/jmorales/.helm/starters Creating /Users/jmorales/.helm/cache/archive Creating /Users/jmorales/.helm/repository/repositories.yaml $HELM_HOME has been configured at /Users/jmorales/.helm. Not installing tiller due to 'client-only' flag having been set Happy Helming! Now, there is a few caveats we need to take into account:\n helm does use HELM_HOST environment variable, or you need to use \u0026ndash;host flag on every command. helm requires to use a kube context with admin provileges. sudoer accounts are not an option. There is no ENV to specify this, so it will use the current-context defined in $KUBECONFIG by default unless other is specified on the command line.  In minishift, the context for the admin user account by default is named \u0026ldquo;default/192-168-99-100:8443/system:admin\u0026quot;. Note that the ip might depend on your install.\nDeploy a sample application Now it\u0026rsquo;s time to deploy any application, as this has really been the goal of what we\u0026rsquo;ve done so far.\nI will use chronograf as sample application (reasons at the end of the blog), and I will create an OpenShift project for it.\n$ oc new-project chronograf $ helm install stable/chronograf --host $TILLER_HOST --kube-context default/192-168-99-100:8443/system:admin -n chronograf --namespace chronograf NAME: chronograf LAST DEPLOYED: Wed Jul 19 14:29:17 2017 NAMESPACE: chronograf STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/Service NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE chronograf-chronograf 172.30.177.119 none 80/TCP 1s ==\u0026gt; v1beta1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE chronograf-chronograf 1 1 1 0 1s NOTES: Chronograf can be accessed via port 80 on the following DNS name from within your cluster: - http://chronograf-chronograf.chronograf You can easily connect to the remote instance from your browser. Forward the webserver port to localhost:8888 - kubectl port-forward --namespace chronograf $(kubectl get pods --namespace chronograf -l app=chronograf-chronograf -o jsonpath='{ .items[0].metadata.name }') 8888 You can also connect to the container running Chronograf. To open a shell session in the pod run the following: - kubectl exec -i -t --namespace chronograf $(kubectl get pods --namespace chronograf -l app=chronograf-chronograf -o jsonpath='{.items[0].metadata.name}') /bin/sh To trail the logs for the Chronograf pod run the following: - kubectl logs -f --namespace chronograf $(kubectl get pods --namespace chronograf -l app=chronograf-chronograf -o jsonpath='{ .items[0].metadata.name }') And you can see it deployed though minishift OpenShift web UI.\nFor convenience, you can wrap the helm command line in a script that will abstract you away the \u0026ndash;host and \u0026ndash;kube-context parameters. But this exercise is left out to you.\nSummary In this blog I have shown you how you can have helm up and running and deploy applications packaged as charts. As I wrote before, I used chronograf as sample application mainly because many of the applications that are packaged as helm charts and shipped in their repositories adole from security considerations. Many of the applications require to run privileged, or with a specific user id. Some others use kubernetes alpha annotations not supported on the latest OpenShift version I was using, and should change to the beta annotation supported (e.g the mysql pvc, where if you don\u0026rsquo;t implicitly specify a storageClass, it uses the alpha version of the annotation).\nThere is a wide range of applications packaged as helm charts and available in the interwebs to use, so now you can easily take advantage of them.\nAs a developer you have just got access to more technology to use. But remember, not always the more is the better.\n","description":"","id":23,"section":"posts","tags":["openshift","origin","development","local","devexp","minishift","build","helm"],"title":"Deploy helm charts on minishift's OpenShift for local development","uri":"http://jorgemoral.es/posts/2017-07-19-deploy-helm-charts-on-minishift/"},{"content":"One of my biggest interests is how to make local development experience with OpenShift as easy as possible. I’m constantly exploring what needs to be enhanced to our current experience as I develop applications for OpenShift very frequently. I work hard to understand developers requirements and eventually provide solutions in the tooling we provide. I use to incubate ideas in a project my team owns, oc-cluster-wrapper. I work very close with our engineering teams to solve these use cases in \u0026ldquo;oc cluster\u0026rdquo; or \u0026ldquo;minishift\u0026rdquo; depending on the nature of the problem, as even if they both can stand up an OpenShift all-in-one instance for local development, they both have different goals.\nMinishift/CDK is the tool that has been created for the end user (a.k.a developer) of OpenShift, where we are trying to streamline and simplify many of the common tasks a developer will do on the platform. It’s an extensible tool, that has recently introduced the notion of \u0026ldquo;addons\u0026rdquo; to allow users and organizations to provide common bootstrapping to their clusters. (I will talk about this in another post).\nIn this post I want to focus on something that really annoys me, as a developer, and I will show the solution I have found for this problem. As always, if the community sees that this is convenient for the general user, the developer, we will make sure that minishift implements this solution out of the box.\nNow, after reading this blog for a minute, you’ll be wondering what’s my problem. Let me describe it.\nThe problem I create and destroy oc cluster\u0026lsquo;s on a daily basis, mainly when I want to work on a branch, on a feature, on a demo, I create a new oc cluster, and when I have finished working on that task, and I know I will no longer work on it, I destroy it. Every time I do this, and I access the OpenShift web UI, I am prompted to accept a certificate. OpenShift’s web UI is exposed through https, and oc cluster uses self signed certificates for this communication.\nOnce the certificate has been accepted, I will not be prompted again. Although the communication is not trusted, I have voluntarily agreed to trust that certificate.\nIf I open a different browser, I will be prompted again to \u0026ldquo;proceed\u0026rdquo; understanding the risks that using a self signed certificate implies.\nNow, the truth is that I’m not accepting any random certificate on the interwebs that who knows who’s managing it. I’m a developer that has created a cluster on my local machine for self use. That means, that at the end of the day what I’m really doing is trusting me, which is something that I usually do.\nThe second problem comes when I look into how \u0026ldquo;oc cluster\u0026rdquo; works, and I see that every time I stand up a new all-in-one instance via \u0026ldquo;oc cluster up\u0026rdquo; I get a new certificate. Even though I may have accepted already a certificate before for a previous instance, I will again be prompted to trust another certificate.\nI’m a mostly Java developer and I’m not an expert on certificates, so after digging a little bit on the internet I learnt that a certificate is signed by what is called a Certificate Authority, which is an entity that is \u0026ldquo;globally trusted\u0026rdquo; that validates that who is using the certificate can also be trusted. This is know as a chain of trust.\nThe process of having a CA validating who you are and what you do so you can be trusted is a complex and costly process, that most of the time is not convenient for ephemeral certificates, those that will live for some time, and are somehow meant for development or testing purposes. That is the reason why \u0026ldquo;oc cluster\u0026rdquo; provides it’s own Certificate Authority (CA) to be able to create all the certificates it will require when creating a cluster. This Certificate Authority is also created with every new \u0026ldquo;oc cluster\u0026quot;.\nWhat can I do then? First and easiest option is to create a CA myself and provide it to \u0026ldquo;oc cluster\u0026rdquo; so that every certificate that get’s created is signed by this Certificate Authority. I can then add this CA to the CAs I trust in my laptop, so I will never be prompted again to accept a certificate that is signed by it. This is fairly easy to do as OpenShift provides a convenience command that can be used to create a CA:\n1 2 3 4 5  $ oc adm ca create-signer-cert \\  --cert \u0026#34;my-ca.crt\u0026#34; \\  --key \u0026#34;my-ca.key\u0026#34; \\  --serial \u0026#34;my-ca.serial.txt\u0026#34; \\  --name=\u0026#34;jorge@localhost\u0026#34;   Then you can just provide this CA to the \u0026ldquo;oc cluster up\u0026rdquo; command line, and it will be used.\n$ oc cluster up --certificate-authority=my-ca.crt This is a really easy solution, but it falls short. This was the time when I got enlightened by one of my colleagues about the risks of this option. If I do globally trust this CA in my laptop and somehow this CA gets leaked from my laptop to someone else, he could just use it malicious purposes that I would blindly \u0026ldquo;trust\u0026rdquo;. This is not a good idea.\nThis colleague explained to me that what I should be doing is creating just a certificate that I should reuse between all my oc cluster\u0026lsquo;s, and that this certificate is what I should globally trust on my laptop. This wouldn’t generate any security risk at all.\nOK, let’s explore this option then.\nOne of the characteristics of \u0026ldquo;oc cluster\u0026rdquo; is that in the bootstrapping process, if there’s already configuration for the instance, it will reuse it (unless it’s not compatible). So, the only thing I need to do is, instead of creating a CA I need to create the certificates that the Web UI will use whenever I access it.\nIt happens that there’s multiple certificates used by an OpenShift instance, and that the CA that will sign these certificates will be different from instance to instance unless reused. So eventually, I need to create the CA and all the certificates, and provide these to every instance I create.\nFor this purpose, there is again an \u0026ldquo;oc\u0026rdquo; command:\n$ oc adm ca create-master-certs \\ --cert-dir=./certs \\ --master=https://127.0.0.1:8443 \\ --public-master=https://127.0.0.1:8443 \\ --hostnames=kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,localhost,openshift,openshift.default,openshift.default.svc,openshift.default.svc.cluster,openshift.default.svc.cluster.local,127.0.0.1,172.17.0.1,172.30.0.1,192.168.65.2 As you can see, this command needs quite some different information. I will indicate the directory where to leave all the certificates, then the API server’s internal and external URLs and a list of hostnames or IPs that the server certificates should be valid for. This last part is the only challenging one, as it requires me to reuse the same IPs and names to be used for the cluster as part of the \u0026ndash;public-hostname argument at bootstrapping time.\nNow, the only thing I need to do is, everytime I bootstrap a cluster I need to provide to it all these certificates. In order to do that, I place all these certificates on a well known location on my laptop and then we need to instruct oc cluster\u0026lsquo;s bootstrapping process to preserve the configuration and to use this \u0026ldquo;well known\u0026rdquo; location for the master’s config files.\nThe following options will be required whenever I start an oc cluster:\noc cluster ... --host-config-dir=DIR/config --use-existing-config Every cluster I bootstrap from this point will use the same certificates, so the only part left is to instruct my local development system (a.k.a my laptop) to trust this certificate for https communication.\nAs I use mac, I will register my certificate using KeyChain. You can directly double-click on the certificate you want to add. In this case will be master.server.crt, and then trust the certificate. You can see the process in the following animated image. (There’s a similar process for every Operating System).\nFrom this point, you will never be asked about this certificate by your browser, and you can create as many clusters as you want, as long as you share this certificate to all of them.\nMaking it simple As you have probably seen, making this tip work with \u0026ldquo;oc cluster\u0026rdquo; is not trivial. For that, my team has been working on a script that simplifies working with \u0026ldquo;oc cluster\u0026rdquo; locally really simple. The script is called \u0026ldquo;oc-cluster\u0026rdquo; and works in the same way as the base command \u0026ldquo;oc cluster\u0026rdquo; but in a simpler way.\nAll these boilerplate that I have presented here is already built into that script, and you can just create a cluster by:\noc-cluster up The only step left to you is to trust the certificate. If you do it, the burden of being asked for every cluster you create will be removed.\nAll the convenience this script provides (which is not the topic of this blog), is used as incubation for developer usability requirements that will get introduced into minishift, meaning that eventually minishift will support similar functions.\nWrap up Here I have presented a way of making your local development experience easier by avoiding you the naggy behaviour of having to accept the self signed certificate every time you create a cluster with oc cluster.\nThere are a lot of small improvements one can make to improve their day to day experience, and that is what we are exploring and contributing into minishift. Minishift is the most streamlined way of working locally with openshift clusters for developers and what I recommend all of you to use, although some of the ideas we experiment in \u0026ldquo;oc-cluster\u0026rdquo; are not yet implemented, but they will soon be.\nBut remember, minishift is not a \u0026ldquo;production ready cluster\u0026rdquo; or a tool for \u0026ldquo;operations\u0026rdquo;. It is just meant to be used as the cluster where developers will test their applications early in the process. This is very important to remember. The experience in minishift is and will further be streamlined for development.\n","description":"","id":24,"section":"posts","tags":["openshift","origin","development","local","devexp","minishift","build"],"title":"Enhancing the local development experience. Trusting your self-signed certificates","uri":"http://jorgemoral.es/posts/2017-07-17-trust-your-certs/"},{"content":"OpenShift provides different options for building and deploying containers on the platform. These generally include:\n   Build and deploy from application source code - Users can specify the location of their source code in a GIT repository. OpenShift will build the application binaries, then build the container images that include those binaries and deploy to OpenShift. Users can also specify a dockerfile as the source code to build container images from.\n  Build and deploy from application binaries - Users can also specify the location of their application binaries, coming from their existing application build process and tools. OpenShift will just build the container images that include those provided binaries and deploy to OpenShift.\n  Build outside of OpenShift - Users can build their applications and container images completely outside of OpenShift, coming from their existing application and container image build process and tools, and specify the location of those images to pull in. OpenShift will just deploy those container image as provided.\n   Enhancing your Builds on OpenShift: Chaining Builds. As previously described, OpenShift provides a mechanism to bring your applications to run in containers on the platform, while abstracting much of the detail of the underlying container runtime, Kubernetes orchestration, and platform itself. This mechanism is called s2i (source-to-image) which uses builder images to build your applications in containers. A builder image is a standard Docker/OCI image that contains additional builder scripts which can build your applications from source or binaries. In the case of Java, the builder images use Java build tools like Maven or Gradle to build an artifact type (jar, war, or ear) and will layer that on a java runtime (JDK, Tomcat, JBoss EAP, Wildfly-Swarm,\u0026#8230;\u0026#8203;) and the end result will be packaged as a new container image for your application and deployed as a container.\n In addition to the typical scenario of using source code as the input to a build, OpenShift build capabilities provides another build input type called “Image source”, that will stream content from one image (source) into another (destination).\n Using this, we can combine source from one or multiple source images. And we can pass one or multiple files and/or folders from a source image to a destination image. Once the destination image has been built it will be pushed into the registry (or an external registry), and will be ready to be deployed.\n   Using an image (or multiple) as the source of the content you want to stream into the destination image may appear unnecessarily complicated, but it opens the door to splitting the build process into two (or even more) different stages: build and assemble.\n The build stage will use a regular source to image process and will pull down your application source code from Git and build it into an application artifact, publishing a new image that contains the built artifact. That’s the sole goal for this stage. For this reason, we can have specialized images that will know how to build an application artifact (or binary) using building tools, like maven, Gradle, go, …\n The assemble stage will copy the binary artifact from the “source” image built in the previous stage and put it in a well known location that a runtime image will use to look for this artifact. Examples of such images could be Wildfly, JBoss EAP, Tomcat, Java OpenJDK, or even scratch (a special image without a base). In this stage we will just need to indicate where the artifacts are located in the source image and where they need to be copied in the destination image. The build process will create an image that will be pushed into the registry and will be known as the application image.\n   Now, I’m going to demonstrate this process with three different examples, that will give us the following benefits:\n   Build the app binaries with a regular s2i builder image and run the app using a vanilla (non s2i) image as the base image.\n  Build the app binaries with a custom s2i builder image with a build tool (like Gradle), and run the app using an officially supported s2i image as the base.\n  Make a minimal runtime image (which has many side benefits).\n     Note  At the end of each example there is the complete code snippet that you can use to reproduce the example in your own environment.       Example 1: Maven builder + non-s2i Wildfly Runtime In this example I will use as runtime image a vanilla wildfly image, which will give me a smaller final image size compared to the s2i version of wildfly. I will use the community version of wildfly available at Docker Hub.\n   I’ll use minishift to start a local OpenShift cluster on my laptop to run these examples, but any OpenShift environment will work.\n I’ll start my minishift environment:\n   Once I have my local environment up and running, I’ll create a new-project to isolate all the changes I do:\n   In this example, I’m going to chain two builds. The first build will use any of the available java based s2i builders in OpenShift, as I only want to build my Java artifact using maven. I’ll use the s2i-wildfly builder image, and will build a sample Java application which I have available in GitHub. Additionally I’ll give this build a name. Let’s keep it simple and call it “builder”.\n   Once the build has finished, which you can verify by watching the build log, I’ll create a second build that will copy the generated artifact from the first build into the second. This second build will use jboss/wildfly image as base, and will copy the ROOT.war artifact from the builder image into the appropriate location. This second build will be a docker build and not a source build, like the previous. I’ll give this build a representative name again. This time the name will be “runtime”.\n   Now I already have my runtime image built, with the application artifact. The only thing missing is to have the application deployed, so I’ll start a new-app from the “runtime” image, and will give it again a meaningful name, “my-application”. Then, I’ll create a route and verify that the application is up and running.\n   This is a simple example where I’m using a non-s2i image to run my application built in OpenShift. I could have used any Docker image, it doesn’t need to be jboss/wildfly, but I used this one since you already know where I work ;-)\n You’ll see this application like any other application on the OpenShift Overview.\n   The main difference is that your application will have two builds, and the application itself, the code, will be built by the “builder” build, in case you want to set a GitHub webhook for your source code.\n   If you want to exercise all the code yourself, you only need to copy and paste the following snippet, which is also available in GitHub.\n oc new-project maven-jbosswildfly oc new-build wildfly~https://github.com/OpenShiftDemos/os-sample-java-web --name=builder # watch the logs oc logs -f bc/builder # Generated artifact is located in /wildfly/standalone/deployments/ROOT.war oc new-build --name=runtime --docker-image=jboss/wildfly \\ --source-image=builder \\ --source-image-path=/wildfly/standalone/deployments/ROOT.war:. \\ --dockerfile=$'FROM jboss/wildfly\\nCOPY ROOT.war /opt/jboss/wildfly/standalone/deployments/ROOT.war' oc logs -f bc/runtime # Deploy and expose the app once built oc new-app runtime --name=my-application oc expose svc/my-application # Print the endpoint URL echo “Access the service at http://$(oc get route/my-application -o jsonpath='{.status.ingress[0].host}')/”   Let’s now explore a different use case for which chained builds can be helpful.\n   Example 2: Gradle builder + JDK Runtime What happens when you want to to run your application with our officially supported OpenJDK image which has been created to run your Java based microservices, but your source code needs to be built using “Gradle”, which is not available in that image?\n In this example I will leverage a builder image I created with support for Gradle (jorgemoralespou/s2i-java) for a previous post, and then, as in the previous example, I will copy the generated artifact into the official openjdk18-openshift image.\n For brevity I will only paste the snippet that does all, as the process was already explained in the previous example.\n The only caveat to this process is that you need to know where the built artifact is left in the builder image and where you need to place the artifact in the runtime image.\n oc new-project gradle-jdk oc new-build jorgemoralespou/s2i-java~https://github.com/jorgemoralespou/s2i-java \\ --context-dir=/test/test-app-gradle/ --name=builder sleep 1 # watch the logs oc logs -f bc/builder # Generated artifact is located in /wildfly/standalone/deployments/ROOT.war oc new-build --name=runtime \\ --docker-image=registry.access.redhat.com/redhat-openjdk-18/openjdk18-openshift \\ --source-image=builder --source-image-path=/opt/openshift/app.jar:. \\ --dockerfile=$'FROM registry.access.redhat.com/redhat-openjdk-18/openjdk18-openshift\\nCOPY app.jar /deployments/app.jar' sleep 1 oc logs -f bc/runtime # Deploy and expose the app once built oc new-app runtime --name=my-application oc expose svc/my-application # Print the endpoint URL echo “Access the service at http://$(oc get route/my-application -o jsonpath='{.status.ingress[0].host}')/”   We have created two different builds, one for building my application and another one for creating the runtime application.\n   The deployed application can be seen in the overview page.\n   Clicking on the route you’ll see the cool example in action.\n   As can be seen, in the process, there are 4 ImageStreams involved:\n   The two base images used, s2i-java for building using Gradle, and openjdk18-openshift to be used as base for running our application. Also there is a builder and runtime ImageStream as result of our builds. Our deployment is based on the “runtime” ImageStream.\n Now that we’ve seen how to use a different builder technology than the available in the images we want to run, let’s explore a final example on how to get a minimal runtime image.\n   Example 3: S2I Go builder + Scratch Runtime Go is a language where you run a “standalone” binary that can be statically compiled to have all the dependencies it requires. In this way, you can run a minimal image with a go binary that is easy to distribute.\n As there is no official go-s2i image, I have modified the one available in GitHub to statically build a binary. The source code for this image is available in GitHub and the image is published in Docker Hub under jorgemoralespou/s2i-go. Keep in mind this image has been built just to prove this use case and that given my lack of expertise in go, you shouldn’t trust it (or use it) for anything important.\n I have an example go application that is a web server showing a hello-world in GitHub, and will be used for this third example.\n As before, and given that the process is the same, I’ll just paste the code snippet that you can copy and paste in your terminal to verify yourself.\n oc new-project go-scratch oc import-image jorgemoralespou/s2i-go --confirm oc new-build s2i-go~https://github.com/jorgemoralespou/ose-chained-builds \\ --context-dir=/go-scratch/hello_world --name=builder sleep 1 # watch the logs oc logs -f bc/builder # Generated artifact is located in /opt/app-root/src/go/src/main/main oc new-build --name=runtime \\ --docker-image=scratch \\ --source-image=builder \\ --source-image-path=/opt/app-root/src/go/src/main/main:. \\ --dockerfile=$'FROM scratch\\nCOPY main /main\\nEXPOSE 8080\\nENTRYPOINT [\"/main\"]' sleep 1 oc logs -f bc/runtime # Deploy and expose the app once built oc new-app runtime --name=my-application oc expose svc/my-application # Print the endpoint URL echo “Access the service at http://$(oc get route/my-application -o jsonpath='{.status.ingress[0].host}')/”   Once the process has finished, we can compare the size of the images. The builder image would be my application image if I wouldn’t have chained into a new build. The runtime image, as it is based off SCRATCH and has just the statically built binary, is 150x smaller in size.\n     Make it simple, make it repeatable Now that we have set up 3 different use cases to which chaining builds can provide some benefit, we can abstract all these complexity in a template, so we just need to instantiate a template providing the location of our source code repository and the name of our application.\n   Additionally we can augment this template with any parameterization we might want to make configurable.\n It is also important to note that using some of the building capabilities provided by OpenShift we have set up an ImageChangeTrigger on the second build so there is no need to manually launch both builds. The second build will be started by OpenShift once the first has finished as a result of the new image being created by the first build.\n Using a template simplifies your user experience and provides you a mechanism to create this type of applications with a single command:\n oc new-app go-scratch \\ -p name=my-application \\ -p GIT_URI= https://github.com/jorgemoralespou/ose-chained-builds \\ -p CONTEXT_DIR=/go-scratch/hello_world     Conclusions To conclude this article, I want you to think about all the capabilities that the platform provides and that sometimes are not obvious to us. With this technique, we can do much more fancy things, that I will show in a follow up blog.\n Also, as many of you would have probably figured out, there’s not only benefits in what I just showed. There will be two docker images being built, pushed and stored in the registry and there will be a bigger maintenance burden. But, the most important thing to understand is that the platform does not limit us in many ways that we could have thought of.\n As always, the complete content used for this blog is available in GitHub.\n I hope that this has given you some food for thought. Happy to chat about it.\n   ","description":"","id":27,"section":"posts","tags":["openshift","origin","development","local","devexp","minishift","build"],"title":"Enhancing your Builds on OpenShift. Chaining Builds.","uri":"http://jorgemoral.es/posts/2017-04-19-chained-builds/"},{"content":"We finally get to the last post of the series, and in this post I will introduce you to the tool that Developers will be using soon. It’s still not final and yet many features need to be planned and included, but will overcome all the problems I described in my previous posts.\n There’s already a good blog post from Lalatendu Mohanty about what CDKv3 is so I will not go into many details. I will just simply quote him for what I consider the most important part:\n “Minishift is a fork of the Minikube project and uses libmachine to interact with the underlying virtualization software. It also uses OpenShift’s “cluster up” functionality for provisioning the local Origin/OCP cluster”\n Also:\n “At the moment, KVM, Virtualbox, Xhyve and HyperV are the supported hypervisors.”\n You should note that from this 2 sentences there is so many relevant things to mention:\n First, and foremost, the tool runs in a virtual machine, providing isolation from the host. Also, the support for the most common virtualization technologies across all major Operating Systems, providing consistency on where minishift can be run. And finally, minishift uses internally “oc cluster up” which is the preferred way to bootstrap a local cluster.\n One of the key aspects of minishift is that it is a tool created with developers in mind, so most of the shortcomings that “oc cluster” provides are meant to be overtaken by minishift. But, not only is a tool for developers, it’s also a tool to have reproducible OpenShift local environments. This aspect is key for some alternative use cases, like teaching or showing OpenShift capabilities in a reproducible manner. Evangelism of OpenShift will greatly benefit from the sweetness that minishift provides.\n As an example of things that are or will be shortly possible:\n   Reuse downloaded images from VM to VM\n  Provision a set of addons/bundles upon cluster creation\n  Modify cluster default behavior\n  Provide multiple openshift instances/profiles\n  Ability to pack and transfer files required to have a working environment\n   How to get started To work with minishift is as easy as it is to work with “oc cluster”. It’s a single go binary compiled for the different major developers operating systems (Windows, Mac and Linux). The only pre-requirement is that you have any of the supported virtualization technologies available on your workstation. Once you have the binary downloaded, and for convenience, added to the path, you can just issue one command:\n $ minishift start   This command, which accepts a variety of configuration flags, does all you need to have OpenShift up and running. It will pull down a boot-2-docker iso (there’s also Centos and RHEL variants), it will create a VM using that iso image, and will do “oc cluster up” within the VM, with the appropriate configuration. After all the images have been pulled down (which can take a while) you’ll have a complete OpenShift all in one cluster running in a VM.\n Once you finish working, you just need to stop the VM, by doing:\n $ minishift stop   Note that this will keep the VM in your workstation, so you can start and stop it several times being sure that your work will be preserved.\n Once you’re done, you can discard the VM and recover all used disc space, by doing:\n $ minishift delete     Conclusions Although minishift has not even hit version 1.0.0 GA it is already one of the easiest ways to work with Openshift locally. But the truth is that if the present is encouraging the future is promising, and we will see a great deal of capabilities being added moving forward that will keep simplifying developers life, so that a regular developer will not need to know how to install, manage, configure and operate and OpenShift cluster and will be able to keep focus in what should be important to him, develop applications that will ultimately run on an OpenShift cluster.\n I have to say that these past months where I’ve been working with the minishift team, they have proved that “being the tool of choice for developers” is their main goal, and they have been listening to all the feedback from the community to make the experience as easy as possible. I want to thank Hardy Ferentschik, Lalatendu Mohanty, Praveen Kumar, Budh Ram Gurung, Gerard Braad, and the rest of the minishift team, and also specially Jimmy Dyson who started this project.\n   ","description":"","id":28,"section":"posts","tags":["openshift","origin","development","local","devexp","minishift","cdk"],"title":"Developing locally with OpenShift - minishift, bringing it all together","uri":"http://jorgemoral.es/posts/2017-04-08-developing-locally-openshift-minishift/"},{"content":"At the time OpenShift started, we realized that having a local development environment was important to make iterative development work more agile. Back then, the requirements that we had for a local development environment were pretty clear:\n   It needs to work on linux, mac and windows\n  It needs to be easy to run\n  It needs to be easily disposable\n  Resources used need to be adjustable\n   There was an existing technology that was meeting all these requirements, Vagrant by Hashicorp. Vagrant is a tool that allows you to start a VM, from a template, and then provision/bootstrap the VM on first boot. Then you could start and stop that VM for as many times as you want/need before you discard it. This technology provided us a way to give a base VM image, based out of Centos or RHEL, that was on first boot bootstrapped to contain a full OpenShift environment.\n The OpenShift evangelist team started this work and created what we called the “Origin all-in-one” which bootstrapped you an OpenShift Origin all-in-one node with additional content. This effort also started based on the necessity of the team to provide a way for developers to have a VM they could take home in order to learn OpenShift.\n The effort lead to an official variant, called CDK (Container Development Kit) and supported by our Red Hat Developers Tools, that was based on Red Hat Enterprise Linux and installed the Enterprise version of OpenShift, now called OpenShift Container Platform. It provides mostly the same capabilities as the all-in-one, but the most important fact for this variant is that Red Hat supports it. You just need to subscribe the VM (free subscription for developers) and you’ll have access to our Enterprise product.\n The downsides to using Vagrant is that you need to use a full Operating System VM that required an initial download of some GBs of data prior to being able to start working. Also it requires a considerable amount of resources not always easily available in the developer’s workstations or laptops.\n Relying on Vagrant seemed the right approach at the time, and in the case of CDK, Red Hat invested in having a team of developers working upstream on some of the plugins used. The truth is that after some time, Vagrant seems more an abandoned project where every release breaks a feature and introduces incompatibilities with some of the versions of the hypervisors, e.g. VirtualBox which makes it complicated for the users to have it properly installed and functional.\n I’m not going to explain how this approach works. If you’re interested you can just look at our docs.\n Even though we could admit that the Origin all-in-one VM has been very successful, it has become less and less the defacto way of running a local OpenShift instance, and it will no longer be maintained, as we have announced. But don’t be afraid, that doesn’t mean there will be no solution. In the following posts I’ll be talking about more options, so stay tuned!\n ","description":"","id":29,"section":"posts","tags":["openshift","origin","development","local","devexp","allinone","cdk"],"title":"Developing locally with OpenShift - Origin all in one. Where we started","uri":"http://jorgemoral.es/posts/2017-04-06-developing-locally-openshift-origin-all-in-one/"},{"content":"OpenShift v3 launched a year and a half ago and during this time we’ve been looking at different ways to run a local OpenShift development environment on the developer’s laptop. In this series of articles I will be introducing the options we have been providing and most importantly, I will describe the evolution in our approach..\n But before digging into any particular solution, we need to set the ground rules of what a local development environment is and why it is important.\n OpenShift is a Cloud Container Application Platform that will usually be installed on a datacenter for enterprise grade usage. This means that developers will have access to a set of nodes that will be acting as development environment for them. As you probably know, OpenShift runs applications packaged as Docker containers, so all that is required to use the platform is the Docker images to run. How you get to these images is not relevant for OpenShift, although it provides some mechanism out of the box that makes your life easier, what is really relevant is that your Docker image is available to the platform.\n As a developer, how can you make this happen? These are some options.\n   Provide source code to the platform and let the platform build it. There are 2 options:\n  Provide your application’s source code and let OpenShift build the Docker image using s2i.\n  Provide a Dockerfile and let OpenShift build the Docker image using a Docker build for you.\n     Provide just your application binary to a special image that knows how to run it. This can be done in different ways but most likely:\n  Provide your application binary to OpenShift and let OpenShift build a Docker image with the binary on top of an existing base image.\n  Provide your application binary as a parameter to your deployment that will use a generic Docker image and will pull it down at startup time. This goes against the model that the Docker image should container everything it needs, but still, is an option.\n     Create a Docker image and push it to the platform. There is multiple ways to create this Docker image, build it locally or in a CI server running internally or externally to openshift.\n  Pull down into the platform an existing Docker image. The only difference is that in this case, you will not be building the image.\n   But let’s step back a little. Since we are developers, it is quite important to us how we create those Docker images. The truth is that, as developers, we will be building our application and trying it out on the environment several times during the development cycle. While doing this using a clustered platform, remote to our laptops, is probably the easiest in terms of convenience, but the truth is that for most developers, like me, this will not be enough. I will probably be building the application several times a day, sometimes tens or even hundreds of times. I will probably like to debug the running application. I will probably want to do many other things that can be summarized in:\n “I want to develop applications for OpenShift as fast as I can develop an application running on my local development box”\n That probably means that\n   I need to be able to have the OpenShift environment as close and available to me as possible\n  I want to have a streamlined process for developing against this local OpenShift environment.\n   While you may not agree with what I just said, I can provide some good reasoning on why I would like this kind of development environment. Of course there’s always some drawbacks, but I think they can be overcome. Let’s start with the pros on why it is good to have an OpenShift local.\n   Independence of location. I no longer need to be attached to the company’s network to be able to work.\n  Independence of connectivity. I will no longer require an internet connection to work. This is not always true, but is true as long as you have your dependencies and base images locally available. And this will work especially well if you integrate your image repository or source code repository into the local OpenShift. For example, this can be accomplished with Gitlab.\n  Customization of the experience. I can customize the build experience so that it looks as close as possible to developing without OpenShift.\n  Faster development cycle. I can iterate faster on my development, code, test, deploy, code, test, deploy, debug,\u0026#8230;\u0026#8203;\n   And now, the few cons I can think of:\n   Solutions for multiple platforms. Developers might use Windows, Linux or Mac, so the local development environment needs to run on these Operating Systems.\n  Developer box resource requirements. Having a local OpenShift install means that the Developer Workstation needs to have enough resources to run it.\n  Administration rights to install the tools. Sometimes we have found that developer’s workstations are standardized and they are not allowed to install additional software.\n   Of these cons, the first one is Red Hat’s responsibility, as we should provide a solution for local development targeting these different environments. The others are something that Enterprises will need to think about when adopting new technology. As we are already in 2017 we need to understand that developers will not be productive using low resources workstations, it’s penny-wise pound foolish. I hope that at this point in time, enterprises understand that specs for developer workstations should no longer be such a big issue.\n There’s always alternatives to running a local OpenShift instance on the developer’s box, but I can not advocate for any solution that goes against the principles of test your production code as soon as possible, which for me, that means on my workstation.\n Most importantly, there is multiple ways to bring your code into an application and that application to run it in your local OpenShift developer instance:\n   Building your application and rsync the compiled code into a running container.\n  Rsync source code directly into a container when using non-compiled languages.\n  Using a maven plugin to build the images on the host and push them into you local OpenShift environment.\n  Many others\u0026#8230;\u0026#8203;\n   Any option you take, should mean that at the end you, as a developer, can test your application running in an OpenShift environment. If on the contrary, you build and verify/test your application in a specific way that will be different as how the application will be run in production, there’s many chances that issues might occur. This should be avoided as much as possible if you want to fully embrace DevOps and agility.\n Now that I have presented the importance of having a local OpenShift environment available in your workstation, I’ll present the evolution of the “local OpenShift development environment” since we started to the present time, when I can say we have finally a really good solution that only can get better.\n In the next blogs I’ll talk about the Origin all-on-one VM (and the Container Development Kit/CDK) using virtualization and Vagrant. Then I’ll move into a pure docker solution with “oc cluster up” and I’ll end up with minishift, a lightweight virtualized option.\n ","description":"","id":30,"section":"posts","tags":["openshift","origin","development","local","devexp","oc-cluster","allinone","minishift","cdk"],"title":"Developing locally with OpenShift - State of the art","uri":"http://jorgemoral.es/posts/2017-04-05-developing-locally-openshift-state-of-the-art/"},{"content":"In this blog, I\u0026#8217;m going to describe what are my requirements when looking for a way to develop locally using OpenShift, and I\u0026#8217;ll describe a tool I have created to help me with this workflow.\n First of all, I have to say that I\u0026#8217;m not only a developer, so maybe the workflow I\u0026#8217;m looking for is too complex, so I\u0026#8217;m still experimenting to refine not only the workflow but also the tooling. I\u0026#8217;ll give some comments on what I would like from my ideal tool.\n Also, I need to say that this workflow is to work on OpenShift, as a developer, as an evangelist, as a product manager, as a tester, as a customer, as a user and the many roles I have throughout my day.\n I\u0026#8217;ll use oc-cluster as the name of the command to show my needs. Why I used this name? Read to the end and I\u0026#8217;ll give you an answer and also a reason why the name will eventually change.\n First, I want an easy way to start and stop an OpenShift install locally, on my laptop.\n oc-cluster up oc-cluster down   I\u0026#8217;m somehow biased by some of the option names used by Vagrant, as I\u0026#8217;ve been a Vagrant user for many years, and still am, and I think some of the command names fits really well to be self descriptive.\n One important thing for me is that the cluster I create will survive multiple days, as I will be using it for a while, so I need it to be persistent. That means that if I need a new cluster, I\u0026#8217;ll have to delete the current one, with:\n oc-cluster destroy   Another important factor for me as I\u0026#8217;m multitasked, is to be able to have different clusters, with different stuff, that I can switch between them at will, so I have used the term profile for this, and when starting, stoping and destroying a cluster I can use the profile name.\n oc-cluster up java oc-cluster down oc-cluster up demo oc-cluster down oc-cluster destroy demo   While I can have everything in one single cluster, in different projects, if I end up adding a lot of applications to a cluster, the amount of resources I would use, and the time it would take to start would be high, so I\u0026#8217;d rather split my clusters into different responsibilities. I can create a cluster for Java development, configure it with a nexus artifact repository manager, and have templates configured to always use nexus, or have a buildOverrides for that. If I started a demo cluster, I would not be interfered by the work I do as a Java developer.\n Next, as there can be multiple profiles, I need a way to know what profiles I have created, so I can pass the appropriate name to the up command:\n oc-cluster list   Also, it\u0026#8217;s very important to know if I already have a cluster up and running and which cluster is it:\n oc-cluster status   So far, this is a basic workflow to start and stop, create and destroy, list and status the clusters I have.\n One important note is that since the create and start command is the same, the command needs to make sure that the first time is invoked will create the cluster, but the subsequent calls will just bring it up, and not do any creational steps.\n Now, some of the needs that as a developer on OpenShift I have are to work with persistence volumes, so there are helper commands that helps on that goal.\n oc-cluster create-volume volumeName [size|10Gi] [path|$HOME/.oc/profiles/{profile}/volumes/{volumeName}] oc-cluster create-shared-volume project/volumeName [size|10Gi] [path|$HOME/.oc/volumes/{volumeName}]   The first command will create an OpenShift PV, that is only available to the profile in use, that means that whenever I destroy the cluster represented by that profile, all the date stored in that volume will be removed as well. Like every developer, I expect that all the defaults are good enough for me to use, so I limit the amount of required params to just the name. This could also be reduced to use random names, and it\u0026#8217;s something I\u0026#8217;ll think about.\n The second command, bring the difference that provides a shared storage between clusters. This way I can mount the same volume on multiple clusters. The main use case I have so far is to be used by persistent applications that I use in many clusters and that are used as infrastructure type of services. A good example for this is nexus artifact repository manager, where I want every dependency that I have already pulled down to be available to every cluster I use.\n Also, talking about OpenShift, I will probably want to get into the OpenShift runner. What I mean with the OpenShift runner is the place where OpenShift is actually running, whether this is a VM or a docker container, and I want to do this in a consistent way and in a way that I will understand. For this I use:\n oc-cluster ssh   Now, I\u0026#8217;ll talk about some of the problems that my clusters could suffer and some solutions to those:\n   Adding functionality to the tool.\n  Bootstrapping users\n  Bootstrapping the cluster\n  Configuration bootstrap\n  Deployments bootstrap\n     Reproducibility\n  Packaging/Transport\n  Using proxies\n   I\u0026#8217;ll try to dive into all these topics, one by one.\n Adding functionality to the tool Sometimes I find that what I can do, or what I want to do, is not covered by the tool, and I understand that also my use case can be quite different from other\u0026#8217;s use cases. To solve this, the tool needs to support the ability to provide plugins that will help on different tasks. These tasks can be:\n   Bootstrapping tasks. Adding stuff to the cluster. Whether it\u0026#8217;s configuration or deployments.\n  New commands. Having commands that extend how the tool works, and that are not considered bootstrapping but more management.\n     Bootstrapping users Users is a key part of local development environments, but it\u0026#8217;s more important when you want to use the local development environment for demos, as you\u0026#8217;ll probably want to have different users, with different roles and belonging to different projects, so you can show things like application promotions, or the difference way of working of a developer compared to an administrator. For me, one of the most important parts, now that it is possible, is to have my regular user act as a sudoer, so I can just execute admin commands if I need to without needing to change user.\n This can be done by adding your user the following:\n oc adm policy add-cluster-role-to-user sudoer developer     Note  My user is developer.     Also, I will probably need to log in as administrator to the web interface, so I need a full user with cluster-admin role.\n oc adm policy add-cluster-role-to-user cluster-admin admin     Note  This action needs to be done as a system:admin, but the tooling we\u0026#8217;ve built take care of this.     Additionally I want to be able to create/delete users, and force them roles, so I can bootstrap a demo cluster for things like application promotions. I need some simple commands that will be able to bootstrap whatever is needed for me, no matter what underlying identity provider is used.\n oc-cluster create-user {username} {role} [{project}] oc-cluster remove-user {username} oc-cluster login {username}   You\u0026#8217;ll probably be wondering why there\u0026#8217;s a login and logout commands. This are needed as usually one can have multiple clusters created, with the same configuration, but sadly every cluster you create will have it\u0026#8217;s own self signed certificates for authenticating. There\u0026#8217;s a need to set in the local oc context the proper cluster and certificates, to avoid errors. This is handled by the login command. As this is a local environment, security is not much of a concern, and passwords can be generalized.\n   Bootstrapping the cluster Following with the things that need to be done are the need to provide some bootstrapping for the clusters, as there will be things I will need always to be provisioned/available in certain clusters. For this, I think that it\u0026#8217;s important to have 2 possibilities, when creating clusters. First of all, is to blueprinting a cluster, so that every time you create a cluster with a certain blueprint, all the bootstrapping will be provided. And additionally, there needs to be a way to bootstrap one-shot clusters, for things you don\u0026#8217;t want so frequently.\n To these, you have blueprints, that will be executed on cluster first bootstrap:\n oc-cluster up {profile_name} {blueprint} oc-cluster up demo pipelines-demo   Or you can do the provisioning afterwards, as a one-shot, as this will be executed in the same way:\n oc-cluster provision {blueprint}   How do you know what blueprints you have?\n oc-cluster blueprints-list   All blueprints can be made composable, so there can be a repository with single actions (enable-pipelines, add-user, add-project, deploy-app) or a composed action (pipelines-demo,msa-demo,\u0026#8230;\u0026#8203;)\n Configuration bootstrap Configuration bootstrapping is the one that requires changes in master or node config\u0026#8217;s file or any other configuration file and that probably will require a restart of the OpenShift process.\n  Deployments bootstrap Deployments bootstrap does not require to restart any process as it will only interact with OpenShift deployable resources, like projects, users, services, routes, deployments, and off course, pulling down all the required images.\n    Reproducibility One of the most important things when developing is that you know that at some point we can screw our environment, and will need to start over. There\u0026#8217;s times where we know the action we\u0026#8217;re going to do can be problematic, and we could probably make a safe point, so if we do something wrong, we can easily revert back the state. This is easy if you just save the configuration to be able to revert back.\n oc-cluster snapshot-save   In the event of a problem, you\u0026#8217;ll might want to go back to a safe configuration:\n oc-cluster snapshot-list oc-cluster snapshot-restore {snapshot-id}     Note  For simplicity, snapshots are made only on running clusters, but can be restored if there\u0026#8217;s no running cluster or the cluster running is the one for the snapshot.     This concept of making save points can be greatly extended, and also is prone to errors if when you restore an environment some images are no longer available. This problem is mostly for self built images. One extension to this concept is having the ability to provide local snapshots, that will be removed if the cluster is removed, and global snapshots, that can be used to recreate a cluster at any moment, and could be made transportable. And this leads us to our next topic.\n   Packaging/Transport Packaging of a cluster is a concept very important for when doing workshops. I do want 40 people in a room to have the exact same thing so I can teach them a lesson and they can experiment themselves. For this, there\u0026#8217;s no easy solution, but as long as they have the tooling, a full cluster can be fully automated for a workshop. How?\n Providing a download link that will do the installation/bootstrapping of all the needed things, a-la installer. So really there\u0026#8217;s no transport, but there\u0026#8217;s a way to bootstrap the same package for everyone.\n This, that sounds really cool and easy, it\u0026#8217;s the most complex of all the tasks, and it is mainly because of the variety of operating systems existing out there. If I just had to focus on mac and linux, it could be very simple, but having to also support windows users it becomes an impossible. At least for me. Hence this requirement is not yet fulfilled.\n oc-cluster install {URL}     Using proxies And last but not least, the support for proxies. This one, that seems easy, is also one of the most complex topics, and this is mostly because the support that OpenShift provides for proxies is not transportable. I\u0026#8217;ll explain myself better. As a developer, I might need to work some time at the office, where I have a proxy to access the internet. At home I might not need the proxy. This scenario is not easily solved in OpenShift, where you\u0026#8217;d need to play with ENV variables being set/unset for every build/deployment every time you move in or out.\n My idea would be something like this:\n oc-cluster enable-proxy {proxy} oc-cluster disable-proxy   But as easy as it looks, I haven\u0026#8217;t figured out yet if this is possible, and how.\n   Existing tooling As I said at the beginning, I\u0026#8217;m using a command called oc-cluster and it\u0026#8217;s because it uses internally OpenShift\u0026#8217;s oc client tool and the cluster option. I found this oc cluster a good way to bootstrap and use a cluster locally on my mac but even on linux as it can use Docker native. If you decide to use docker-machine my command will not work. There\u0026#8217;s many advantages to using docker locally, but there\u0026#8217;s also some disadvantages. Hopefully the disadvantages can be easily solved but the advantages can not be easily taken with other approaches, as when using docker-machine.\n Advantages I see:\n   You can use your local file system for persistent volumes.\n  You don\u0026#8217;t have a virtualization layer.\n  Image are directly available to all the clusters once pulled.\n  You can share volumes between different clusters, even if they are not running.\n   Disadvantages I see:\n   Packaging and transportation with VM could be made easy, but then, there\u0026#8217;s many virtualization out there to make it work on all, or the most important.\n  Removal of built images is easier, as the images are built into the VM. Deleting the VM, deletes all built images. This can also be solved with the tooling, as there is a feature coming that will provide labels to the images created, so every cluster will label their images. Removing the cluster, will remove their images.\n   Probably there\u0026#8217;s more, but these are the ones I can think of right now.\n   Options There are some options out there, some of them more portable, but also, less flexible. As I\u0026#8217;m developing this tool just for me, I focus on something that will work for me, but as I think that most of what works for me could be made work for anyone, I\u0026#8217;m here sharing these thoughts with you.\n Options:\n   CDK, ADB: Using Virtualization through Vagrant and using vagrant plugins. It\u0026#8217;s portable but very heavy weight and outdated.\n  OpenShift.org All-in-one: Same as before. Although it\u0026#8217;s up to date and it doesn\u0026#8217;t use plugins it\u0026#8217;s heavyweight. Even I\u0026#8217;m the author of it, I know it has many limitations, and I\u0026#8217;m just limiting the use of it to those use cases I can not still cover, like doing workshops with Windows users :-(\n  Minishift: It uses lightweight virtualization, but still don\u0026#8217;t provide many of the additional features I need. I would used it and extend it, but since it\u0026#8217;s written in go, I can not contribute to it. I find this a great option, probably the best. Although I don\u0026#8217;t like the name of the commands used, I think will be the way to move forward, and also it is based on minikube, which seems to have adoption on Kubernetes community, which is also great. The maintainer is a great guy.\n  Plain oc cluster: This provides a great foundation, but in no way is something usable per se for developers. Just having a default that makes configuration ephemeral is something that for a developer is not interesting. But as the tool is a great cluster bootstrapper, I use it, and try to ask for features that will make developer\u0026#8217;s use case through our tool more interesting and easy. Also the maintainer is a great guy.\n     Final comments I love OpenShift, I love Kubernetes. I think it is a great platform to run your containers at scale, but I still see that for developers there\u0026#8217;s a steep learning and usability curve. I hope that one day, Java developers (well really any developer) will deploy locally on Kubernetes/OpenShift and not plain docker. Also that they keep developing in plain Java, using their IDEs, building their artifacts or images however they want (s2i, docker build on OpenShift or maven and docker build locally), but that the proces it\u0026#8217;s easy for them to use. I think that for them to adopt a platform like this, the development process needs to be:\n   Easy. Not many additional steps to use the local platform.\n  Fast. It needs to be as fast as without using the local platform.\n  Integrated. They can use the same tools to work on their local platform.\n   This is one of the required steps to have an environment (local platform) to use. Following should be to be easy to collaborate between your local and remote environments. But that, should be the topic of another post.\n As always, if you want to comment, please use tweeter.\n Download the oc-cluster\n   ","description":"","id":35,"section":"posts","tags":["openshift","origin","development","local","devexp","oc-cluster"],"title":"Developing locally with OpenShift","uri":"http://jorgemoral.es/posts/2016-10-20-developing-locally-with-openshift/"},{"content":"In a real world, your applications will be transitioning from environment to environment, from development to testing and into production, as part of their lifecycle. In a container world, applications are assembled into one or many container images, hence what will be promoted are images.\n In this blog I will demonstrate the concepts we learnt about externalizing configuration in your image promotion scenarios. As Veer has previously showed, OpenShift is a platform where we can easily model the concept of stages/environments per application, and we can promote an application (image) from environment to environment just by tagging it accordingly in the project.\n In this example, I will be using the same application I used before, and I will create two projects simulating two different stages/environments for my application:\n   node-app-dev will model the development stage and will be owned by user dev\n  node-app-test will model the testing stage and will be owned by user test\n   This application will be deployed with the exact same BuildConfig in both projects, but for each, a different configuration will be used by means of deploying different values in the ConfigMap. dev user, will have an additional task of building the application from source before deploying it.\n $ git clone https://github.com/jorgemoralespou/ose-app-promotion-configmap.git $ cd ose-app-promotion-configmap/example2 $ oc login 10.2.2.2:8443 -u dev -p dev $ oc new-project node-app-dev $ oc create -f configmap-dev.json $ oc create -f node-app-deployment.json $ oc create -f node-app-build.json   These commands will create a new project called node-app-dev, as user dev, and will deploy a ConfigMap, with a message and background color that will be used in development environment. Additionally, it will create the deployment configuration for the application and it will build our application from source code.\n Once the process is finished, we will be able to see the result:\n   Now, as user test, we will create a ConfigMap with different contents, reflecting our test environment , but we will use exactly the same DeploymentConfig like before. The reason for that is, we are separating the application from its configuration, by leveraging the ConfigMap resource.\n $ git clone https://github.com/jorgemoralespou/ose-app-promotion-configmap.git $ cd ose-app-promotion-configmap/example2 $ oc login 10.2.2.2:8443 -u test -p test $ oc new-project node-app-test $ oc create -f configmap-test.json $ oc create -f node-app-build.json   In this project, there will be no application deployed because no image has been tagged into the test project yet:\n   There is one security requirement to allow dev user to tag into the test project and for a user in test project to pull down the image from the repository.\n Since we are fans of fine grained security, I will be creating a new role, image-tagger, that will be granted the rights to tag an ImageStream. That role will be assigned to the dev user in the test project. This action needs to be executed as an cluster admin user:\n $ oc login 10.2.2.2:8443 -u admin -p admin $ oc create -f - \u0026lt; \u0026lt; EOF { \"kind\": \"ClusterRole\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"image-tagger\" }, \"rules\": [ { \"verbs\": [ \"get\", \"list\", \"create\", \"update\", \"edit\" ], \"attributeRestrictions\": null, \"apiGroups\": null, \"resources\": [ \"imagestreamimages\", \"imagestreamimports\", \"imagestreammappings\", \"imagestreams\", \"imagestreamtags\" ] } ] } EOF $ oc adm policy add-role-to-user image-tagger dev -n node-app-test   Additionally, we also need the user from test project to be able to pull down the image from dev project. But since in OpenShift, by default, the deployment is done by the deployment ServiceAccount, we need to assign the role accordingly\n $ oc adm policy add-role-to-user system:image-puller system:serviceaccount:node-app-test:deployer -n node-app-dev   Now that all the required permissions are in place, we can have dev user to promote our application. For this he will just tag the image in the node-app-test project, and it will be automatically deployed.\n $ oc login 10.2.2.2:8443 -u dev -p dev $ oc tag node-app-dev/node-app:latest node-app-test/node-app:latest   You can now verify that the image has been promoted, and is now running in the testing environment, showing the configuration for this environment.\n   In this blog we have demonstrated a way of separating configuration from application so that the process of promoting an application gets easier, requiring almost none customizations of the base resources being deployed.\n This blog example can be fully executed in the Openshift Origin all-in-one Vagrant image, by doing:\n $ git clone https://github.com/jorgemoralespou/ose-app-promotion-configmap.git $ cd ose-app-promotion-configmap/example2 $ oc login 10.2.2.2:8443 -u dev -p dev $ oc new-project node-app-dev $ oc create -f configmap-dev.json $ oc create -f node-app-deployment.json $ oc create -f node-app-build.json $ oc login 10.2.2.2:8443 -u test -p test $ oc new-project node-app-test $ oc create -f configmap-test.json $ oc create -f node-app-deployment.json $ oc login 10.2.2.2:8443 -u admin -p admin $ oc create -f roles.json $ oc adm policy add-role-to-user image-tagger dev -n node-app-test $ oc adm policy add-role-to-user system:image-puller system:serviceaccount:node-app-test:deployer -n node-app-dev $ oc login 10.2.2.2:8443 -u dev -p dev $ echo \"If you want to promote the application, you can:\" $ echo \" oc tag node-app-dev/node-app:latest node-app-test/node-app:latest\"   ","description":"","id":36,"section":"posts","tags":["openshift","origin","config","configmap"],"title":"Configuring your application, Part 2","uri":"http://jorgemoral.es/posts/2016-07-01-configuring-your-app-2/"},{"content":"Kubernetes 1.2, released more than a month ago, has brought many interesting additions to the Kubernetes platform, but there’s one, that relates to configuration management, that’s especially relevant for application developers, this is ConfigMap. In this blog entry I will share some experiences and tips on using ConfigMap that goes beyond what one of our engineers and Kubernetes contributor, Paul Morie, recently blogged about it. We will take advantage of this new feature in a real application that we will be promoting through different environments, from development through testing into production.\n One of the challenges that we face as developers is the need to externalize application configuration as this will probably be different per environment or per deployment. In OpenShift, Docker, and Kubernetes, developers have been externalizing this configuration into Environment variables, allowing every deployment to have different values for runtime configuration. This has been a fantastic way to adhere to the third rule of the 12factor methodology, “store config in the environment”. When you think that what we, as users of OpenShift, are deploying into the platform is not a container but rather an application, that can be composed of one or multiple containers, we understand that the configuration per deployment really spans greater context than the container itself.\n We need a way of having all the configuration used/needed per application centralized, so that changes in configuration will not impact the definition of the deployment. This is what ConfigMap is for.\n The official Kubernetes documentation states:\n  Many applications require configuration via some combination of config files, command line arguments, and environment variables. These configuration artifacts should be decoupled from image content in order to keep containerized applications portable. The ConfigMap API resource provides mechanisms to inject containers with configuration data while keeping containers agnostic of Kubernetes. ConfigMap can be used to store fine-grained information like individual properties or coarse-grained information like entire config files or JSON blobs.\n   So, what type of configuration can we provide to an application or deployment? It will typically fall under one of these categories:\n   Single configuration value expressed as an environment variable for the container.\n  Command line arguments in a container.\n  Multiple configuration values typically set in a configuration file.\n   Let’s explore how we can use ConfigMaps:\n I will demonstrate ConfigMap with a simple node.js app. This application will print a message as body content and will have background color configurable. We will externalize configuration into a ConfigMap. The code for the application is on GitHub.\n First required thing is to create a ConfigMap to hold the configuration values you want to make available to the deployment. My node.js application will get it’s background color from a property named “color” in a file named “/etc/node-app/node-app.config” in the container. It will also, look for an environment property named message for a string to be show as background message. The configuration file contents that will be used is:\n color=blue   Let’s create a ConfigMap, named config, with both a literal text, message=Hello world!, and the configuration file:\n $ oc create configmap config \\ --from-literal=message=’Hello world!’ \\ --from-file=ui.properties configmap \"config\" created   Let’s verify the contents of our ConfigMap.\n $ oc get configmap/config -o json { \"kind\": \"ConfigMap\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"config\", }, \"data\": { \"message\": \"Hello world!\", \"ui.properties\": \"color=blue\\n\" } }     Note  Some internal metadata has been removed from output for brevity.     Now that I have a ConfigMap with the configuration my application requires, I’m going to deploy an application that will make use of it.\n In my sample app, I will consume the configuration provided by the ConfigMap, mapping the ConfigMap property message to the environment variable BACKGROUND_MSG the application expects, and also mapping the ui.properties into a file located in /etc/node-app/node-app.config.\n \"template\": { \"metadata\": { \"labels\": { \"app\": \"node-app\", \"deploymentconfig\": \"node-app\" } }, \"spec\": { \"containers\": [ { \"name\": \"node-app\", \"image\": \"node-app\", \"ports\": [ { \"containerPort\": 8080, \"protocol\": \"TCP\" } ], \"env\": [ { \"name\": \"OPENSHIFT_NODEJS_PORT\", \"value\": \"8080\" }, { \"name\": \"BACKGROUND_MESSAGE\", \"valueFrom\": { \"configMapKeyRef\": { \"name\": \"config\", \"key\": \"message\" } } } ], \"volumeMounts\":[ { \"name\": \"app-config\", \"mountPath\": \"/etc/node-app/\" } ], \"resources\": {}, \"terminationMessagePath\": \"/dev/termination-log\", \"imagePullPolicy\": \"Always\" } ], \"volumes\": [ { \"name\": \"app-config\", \"configMap\": { \"name\": \"config\", \"items\": [ { \"key\": \"ui.properties\", \"path\": \"node-app.config\" } ] } } ], \"restartPolicy\": \"Always\", \"terminationGracePeriodSeconds\": 30, \"dnsPolicy\": \"ClusterFirst\", \"securityContext\": {} } } }   Configuration is assembled at deployment time, so when the application is deployed and there is no ConfigMap that satisfies the DeploymentConfig, we will have a warning event in our Event log that will help us diagnose the misconfiguration that prevented the deployment to start:\n     One important thing to know is, when a ConfigMap is mounted as a volume, we can change the contents of the ConfigMap, and the mounted file in the container will be eventually updated, when the kubelet on the node re-synchs the pod, providing for changes in configuration in running containers. The running application needs to provide a mechanism to reload configuration changes when they happen.\n In this blog we have demonstrated a way of externalizing configuration of an application. Remember, ConfigMaps are GA in Kubernetes 1.2 and OpenShift 3.2 and some improvements are still to come. Just take these simple restrictions into account:\n   ConfigMaps must be created before they are consumed in pods.\n  ConfigMaps reside in a namespace. They can only be referenced by pods in the same namespace.\n   The example shown in this blog can be fully executed in the Openshift Origin all-in-one Vagrant image, by doing:\n $ git clone https://github.com/jorgemoralespou/ose-app-promotion-configmap.git $ cd ose-app-promotion-configmap/example1 $ oc new-project configmap-example $ oc create -f configmap-example.json $ oc create -f node-app-deployment.json $ oc create -f node-app-build.json      See a video in action\n ","description":"","id":38,"section":"posts","tags":["openshift","origin","config","configmap"],"title":"Configuring your application, Part 1","uri":"http://jorgemoral.es/posts/2016-06-09-configuring-your-app-1/"},{"content":"We launched OpenShift 3.0 back in June 2015 and I have had the pleasure of speaking with users all over Europe and the EMEA region to help them get up and running with deploying applications on the platform. One of the features that developers and administrator often ask questions about are Service Accounts and Security Context Constraints. In this blog post, I will provide a simple introduction into both concepts, how they work and their usage.\n   Security Context Constraints (SCC) The official documentation states:\n  OpenShift provides security context constraints (SCC) that control the actions that a pod can perform and what it has the ability to access.\n   In short, when we execute a container, we want to guarantee that the capabilities required by that container to run are satisfied, while at the same time we also want OpenShift to be a secure Container Application platform. For this reason we can not allow any container to get access to unnecessary capabilities or to run in an insecure way (e.g. privileged or as root).\n OpenShift guarantees that the capabilities required by a container are granted to the user that executes the container at admission time. Admission is done based on the identity of the user executing the pod and the pod’s service account (introduced later in this blog).\n The OpenShift Container Application Platform provides a set of predefined Security Context Constraints that can be used, modified or extended by any administrator. The SCCs that can be used are as follows:\n $ oc get scc NAME PRIV CAPS HOSTDIR SELINUX RUNASUSER FSGROUP SUPGROUP PRIORITY anyuid false [] false MustRunAs RunAsAny RunAsAny RunAsAny 10 hostaccess false [] true MustRunAs MustRunAsRange RunAsAny RunAsAny \u0026lt;none\u0026gt; hostmount-anyuid false [] true MustRunAs RunAsAny RunAsAny RunAsAny \u0026lt;none\u0026gt; nonroot false [] false MustRunAs MustRunAsNonRoot RunAsAny RunAsAny \u0026lt;none\u0026gt; privileged true [] true RunAsAny RunAsAny RunAsAny RunAsAny \u0026lt;none\u0026gt; restricted false [] false MustRunAs MustRunAsRange RunAsAny RunAsAny \u0026lt;none\u0026gt;   By default, the execution of any container will be granted the restricted SCC and only the capabilities defined by that SCC.\n $ oc describe scc restricted Name: restricted Priority: \u0026lt;none\u0026gt; Access: Users: \u0026lt;none\u0026gt; Groups: system:authenticated Settings: Allow Privileged: false Default Add Capabilities: \u0026lt;none\u0026gt; Required Drop Capabilities: KILL,MKNOD,SYS_CHROOT,SETUID,SETGID Allowed Capabilities: \u0026lt;none\u0026gt; Allowed Volume Types: configMap,downwardAPI,emptyDir,persistentVolumeClaim,secret Allow Host Network: false Allow Host Ports: false Allow Host PID: false Allow Host IPC: false Read Only Root Filesystem: false Run As User Strategy: MustRunAsRange UID: \u0026lt;none\u0026gt; UID Range Min: \u0026lt;none\u0026gt; UID Range Max: \u0026lt;none\u0026gt; SELinux Context Strategy: MustRunAs User: \u0026lt;none\u0026gt; Role: \u0026lt;none\u0026gt; Type: \u0026lt;none\u0026gt; Level: \u0026lt;none\u0026gt; FSGroup Strategy: MustRunAs Ranges: \u0026lt;none\u0026gt; Supplemental Groups Strategy: RunAsAny Ranges: \u0026lt;none\u0026gt;   As can be seen in the previous description of the restricted SCC, a list of users and groups can be specified. In order to grant a user or group a specific SCC, a cluster administrator can execute the following command:\n $ oadm policy add-user-to-scc \u0026lt;scc_name\u0026gt; \u0026lt;user_name\u0026gt; $ oadm policy add-group-to-scc \u0026lt;scc_name\u0026gt; \u0026lt;group_name\u0026gt;     Service Accounts The official documentation states:\n  When a person uses the command line or web console, their API token authenticates them to the OpenShift API. However, when a regular user’s credentials are not available, it is common for components to make API calls independently. For example:\n   Replication controllers make API calls to create or delete pods\n  Applications inside containers could make API calls for discovery purposes\n  External applications could make API calls for monitoring or integration purposes\n   Service accounts provide a flexible way to control API access without sharing a regular user’s credentials.\n   As you can see, there are many use cases for Service Accounts, and if we dive into the first use case aforementioned, we need to understand that OpenShift (and Kubernetes) are not synchronous in the execution of their commands.\n   When a user wants to deploy an application, he creates a DeploymentConfig resource, which describes a desired state (as can be seen in the picture above - step 1). From this point, a set of controllers (admission, replication controller, scheduler,\u0026#8230;\u0026#8203;), running on the master server, will be monitoring those definitions and will execute necessary actions on the OpenShift platform in order to provide consistency between the desired and actual state (as can be seen in the picture above - step 2). This will happen as soon as the controllers try to consolidate the cluster state.\n What this really means is, the actions are executed by the OpenShift controllers and not by the actual user, that expressed the desired state. This leads to the situation where we need to identify who\u0026#8217;s executing the actions the controllers are invoking.\n By default, OpenShift creates three service accounts per project for building, deploying and running an application (see the official documentation for more details).\n When a user creates anyobject in OpenShift it will use these default Service Accounts, but a different one can be specified within the object configuration.\n { \"kind\": \"DeploymentConfig\", \"apiVersion\": \"v1\", \"metadata\": {...}, \"spec\": { ... \"template\": { ... \"spec\":{ \"containers\": [ ], ... \"serviceAccountName\": \"myserviceaccount\" } } } }   One reason to use a dedicated service account in a deployment configuration is to allow an application running within a pod to use a set of privileges or capabilities other than those granted by the default service account. This default service account will only have access to all the capabilities defined by the restricted SCC, as out of the box OpenShift will add every authenticated user to the restricted SCC (as can bee seen in the output of the execution of “oc describe scc restricted” shown above). This includes the default service account which is not explicitly included in any other SCC.\n Since every service account has an associated username, it can be added to any specific SCC in a similar way as we have done previously with users and groups.\n As an example, we might want to run an application that needs access to mount hostPath volumes, or we might want to run an application with a specified user and not a random user OpenShift will use as default (as detailed in this blog), or we might want to restrict the container\u0026#8217;s filesystem to be readonly, and forcing every write to be on external storage. There are many other situations that might require us to change the capabilities provided by default.\n This leads to the conclusion of this blog with my advice:\n “Every time you have an application/process that requires a capability not granted by the restricted SCC, create a new, specific service account and add it to the appropriate SCC. But, if there is no SCC that perfectly suits your needs, instead of using the best fit one, create a new SCC tailored for your requirements, and finally set it for the deployment configuration (as described above).”\n $ oc create serviceaccount useroot $ oc patch dc/myAppNeedsRoot --patch '{\"spec\":{\"template\":{\"spec\":{\"serviceAccountName\": \"useroot\"}}}}' $ oc adm policy add-scc-to-user anyuid -z useroot   Above you can see my advice in action, creating a new service account named useroot, modifying the deployment configuration for myAppNeedsRoot and then adding the serviceaccount to the anyuid SCC as the application defined needs to run as user root in the container. Note that I haven\u0026#8217;t created a specific SCC since anyuid meets my needs.\n   Note  The previous example is using notation available in OpenShift Origin 1.1.4+ and OpenShift Enterprise 3.2+.     I’ve seen many users granting access to a user/serviceaccount to the privileged SCC to avoid going through this exercise, and this is can be a big security problem, so take my word of caution.\n   ","description":"","id":39,"section":"posts","tags":["openshift","origin","scc"],"title":"Understanding Service Accounts and SCCs","uri":"http://jorgemoral.es/posts/2016-04-15-understanding-sas_and-sccs/"},{"content":"As you might know, OpenShift 3 Enterprise provides Middleware Services (xPaas), which is a set of Java based images for JBoss EAP, JBoss EWS (Tomcat), JBoss Fuse Integration Services, JBoss A-MQ, JBoss Decision Server and JBoss Data Grid. Also, OpenShift Origin provides an additional JBoss based images for Wildfly, our application server community project. All these images are source-to-image (S2I) enable, that means that will get your application source code built (using Maven) and layered into the application container.\n When working with Maven, it is very common to use a Central Artifact Repository Manager in your organization for centralizing and managing all the required and generated dependencies, as well as providing you with isolation from the real location of the artifacts in the Internet and some security mechanisms, amongst other features. During my life as a developer and consultant I\u0026#8217;ve been working with Nexus Artifact Manager for this purpose. I will not say that it\u0026#8217;s the best or worst, but only that it is the one most familiar to me, and because of that, I will be using it in my OpenShift install.\n It is important to note that everything I will describe can be executed in OpenShift Enterprise or Origin, the only requirement is, that if you\u0026#8217;re using the Middleware Services images you should have the corresponding subscriptions for running them.\n The first thing we need to do is to lay out our OpenShift architecture. I\u0026#8217;ve decided to deploy Nexus as a service in OpenShift, for that purpose I have created a Nexus image (not supported) that I will be building and deploying internally in my OpenShift instance, in a project that I\u0026#8217;ve called ci. This project name is important as it will be used to reference the nexus instance. It is part of the service DNS name.\n $ oc new-project ci --display-name=\"Continuous Integration for OpenShift\" --description=\"This project holds all continuous integration required infrastructure, like Nexus, Jenkins,...\" $ oc create -f https://raw.githubusercontent.com/jorgemoralespou/nexus-ose/master/nexus/ose3/nexus-resources.json -n ci    The steps above will create a project called ci, and it will add some OpenShift resources to the project, namely    A nexus ServiceAccount for using in build\n  A BuildConfig for building the Nexus image, based on Centos7, that will be published into a nexus ImageStream. When the BuildConfig gets deployed, a nexus build will be triggered.\n        Note  I\u0026#8217;ve used the official sonatype nexus image\u0026#8217;s Dockerfile as base and extended with my own requirements for the purpose of this blog, like making sure any user will be able to deploy the image with an OpenShift restricted policy, or adding configuration to use Red Hat\u0026#8217;s JBoss mave repositories.     The build will take some time, so be patient!.\n     Both centos7 and nexus ImageStream definitions\n       Two Template`s called nexus-ephemeral and nexus-persistent.\n     The templates that are provided as part of the loaded resources will allow you to deploy an instance of the Nexus image built, using the nexus ServiceAccount, and configured to have a service on port 8081 and a route on whatever hostname you decide, for external access. Also, these templates will allow you to have a persistent instance of Nexus, using a PersistentVolume or working in an ephemeral mode, where if the nexus replica dies, you\u0026#8217;ll lose all of your cached dependencies. For testing purposes, it\u0026#8217;s much easier to setup the ephemeral instance, but for a more real usage, you should consider only the persistent image.\n There is full instruction on how to set the persistent volume and all the requirements in the README file in the Github repository\n In this example, I will deploy the ephemeral version, with the following command:\n oc new-app --template=nexus-ephemeral --param=APPLICATION_HOSTNAME=nexus.apps.10.2.2.2.xip.io   You can also deploy your nexus instance using the OpenShift console:\n   It is very important to understand that the nexus instance will not be deployed until the build process has finished, and this can take quite some time, so be patient!\n     Note  The value provided to APPLICATION_HOSTNAME is dependant on your installation. My OpenShift environment default application domain is apps.10.2.2.2.xip.io     We can access our nexus instance through the APPLICATION_HOSTNAME value we have provided, and check what repositories are in there. Default credentials for this nexus instance are (admin/admin123). It is important to note, that this Nexus server comes already configured with some Red Hat JBoss repositories, to allow our S2I images to fetch the appropriate dependencies.\n   What we need now is a way of instructing our JBoss S2I builder images to use this nexus instance as artifact repository manager. There is some alternatives to this, of which I will show two of them.\n   Using the provided S2I builder JBoss EAP S2I Builder Image version 1.2, which is the latest version of the builder image, that comes with OpenShift Enterprise 3.1, it provides an environment variable that can be set to point to a maven mirror url, unsurprisingly it is called MAVEN_MIRROR_URL. I will use that variable to get the maven artifacts through our Nexus instance.\n To check that our builds will use our internal nexus instance, we can browse to the public group page and verify that there is no dependency currently stored.\n   Let\u0026#8217;s create a new project and create a sample application using nexus.\n $ oc new-project eap-nexus-builds --display-name=\"EAP builds with Nexus\" --description=\"Building Applications in EAP using Nexus for dependency management\"   For the application, we will be using the EAP S2I Builder image, and we will use the default sample project, and we will set a build MAVEN_MIRROR_URL.\n   You should notice that I\u0026#8217;ve used internal DNS name of our nexus instance, which is nexus.ci.svc.cluster.local, which follows the pattern \u0026lt;service-name\u0026gt;.\u0026lt;project\u0026gt;.svc.cluster.local for services. This is a very powerful feature of OpenShift that provides DNS names for every service, and much more.\n When building the application, we will notice that maven dependencies are being pulled from our nexus instance, instead of the default public Red Hat JBoss' repositories.\n   Once our build is finished, we will also see how our nexus repository artifact group is filled with all the dependencies that have been pulled down.\n   And we will have our application running.\n   Here, we can a historical view of the builds before and after setting MAVEN_MIRROR_URL. The first build in OpenShift always takes longer than any other build as it has to push all the base layers to the registry after the build. Successive builds will just push the application layer. From build #2 to #5 we can see the time it takes a normal build, without using Nexus, averaging 1 minute and 13 seconds\n Build #7 introduces the change with MAVEN_MIRROR_URL set, but as this is the first build after the environment variable has been set, it still took 1 minute and 8 seconds to complete. This build was populating Nexus with all the pulled down dependencies.\n In builds #8 to #10 we can see that the average time it takes now to build is 42 seconds\n As can be seen, we get an average benefit of 31 seconds in building time after introducing our integration with an artifact repository manager, like Nexus.\n   Modifying the S2I builder Not always one can have the comfort of working with S2i builder images that expose the ability to set a Maven mirror like the Middleware Services images provided by Red Hat does, in that cases you need to think of other mechanisms to integrate these images with an artifact repository manager.\n The options can vary, ranging from the most obvious, modify or extend the builder image, using incremental builds, up to creating builder image from scratch. Since I do not like modifying existing images, especially those created by others, I will show how to extend existing Wildfly S2I Builder images to make use of a Nexus artifact repository manager. The same approach can be used with any other builder image, and some other technologies that use or can benefit from the use of an artifact repository manager, especially that Nexus or Artifactory support storing dependencies for other languages than just java.\n I have created a file that will install all the required resources needed to work with the Nexus instance provided in the OpenShift install. These resources are:\n   3 BuildConfigs, for Wildfly 8, Wildfly 9 and Wildfly 10.\n  6 ImageStreams, one for each of the original ImageStreams for every Wildfly version (8, 9 and 10) and another one for each of the modified S2I builder images for Wildfly integrated with nexus (8, 9 and 10).\n   The change that I’ve done to the default Wildfly S2I builder image is as simple as providing an overloaded settings.xml file in my custom S2I builder images that points to the nexus artifact repository manager. This change is the easiest to prove this functionality, although probably a better option would be to provide environment variable to customize the assembly process.\n To install the Wildfly version:\n $ oc new-project wildfly-nexus-builds --display-name=\"Wildfly builds with Nexus\" --description=\"Building Applications in Wildfly using Nexus for dependency management\" $ oc create -f https://raw.githubusercontent.com/jorgemoralespou/nexus-ose/master/builders/wildfly-nexus/wildfly-nexus-resources.json   Once we have our custom Wildfly S2I images built,\n   we can just create a sample application with them.\n $ oc new-app --docker-image=wildfly-nexus-9 --strategy=source --code=https://github.com/bparees/openshift-jee-sample.git --name='wildfly-nexus-sample'   Here, we see as well that our build process is fetching the required maven dependencies from the provided Nexus artifact repository manager.\n   This first build took 3 minutes and 11 seconds, it includes building with the plain wildfly-9 image available on Github, and the time needed to pull down the image. This image was not doing any dependency management.\n In the second build, I updated the BuildConfig to use wildfly-nexus-9 builder image and this build took 1 minutes and 24 seconds. The reason for that is that Nexus was caching all the dependencies, since I used a clean nexus instance.\n On the third and fourth build, all the dependencies were already cached in Nexus and build time dropped to 37 and 35 seconds, respectively.\n As in the previous example, with EAP, we get a benefit of more than 40 seconds in our build time by using an artifact repository manager, like Nexus.\n   Using incremental build Another option, I’ve mentioned before, we can use to improve Maven based Java builds in OpenShift is to enable the incremental builds. Unfortunately not all images support this feature, since it requires the existence of save-artifacts script, responsible for saving artifacts used during builds. In our cases these will be maven dependencies. This will have the same behavior as having a local maven repository into the build image itself, with the drawback of reaching out for the previously built image and getting the dependencies out of it.\n To test this mode, I have created a sample resources file that can be easily tested.\n $ oc new-project eap-incremental-builds --display-name=\"EAP incremental builds\" --description=\"Building Applications in EAP using incremental build mode\" $ oc create -f https://raw.githubusercontent.com/jorgemoralespou/nexus-ose/master/other/eap-incremental/eap-incremental-resources.json   After we\u0026#8217;ve created the resources, let\u0026#8217;s do some builds and look at the times.\n   As can be seen in the image above, the times for the second and third build, which are the builds benefiting from the stored artifacts takes much less time, 48 and 47 seconds, but it\u0026#8217;s the same time it takes when using the artifact repository manager, so there is no additional benefit in time, although it is much simpler for those images that support incremental mode, as the developer will only need to specify a flag in the BuildConfig.\n   In this example, the application and pulled down dependencies are not adding a big overhead in size to the initial eap64-openshift S2I image, only 7 MB.\n   But we need to be careful with this approach as there are other images or applications that will have much more dependencies, and the size of the generated image can grow enormously. 130 MB in the following example using Fuse Integration Services.\n     Summary For every application that we build we will be getting a performance benefit by caching into an artifact repository manager it\u0026#8217;s dependencies. Initially we will be perceiving a performance benefit for the second and subsequent builds of every application, but as the artifact repository manager stores more and more dependencies this benefit will be also seen in initial builds of new applications, and most of the dependencies will already be cached.\n Also, we can use incremental builds to get better performance on Java based builds, but it is important to understand that even this approach is easier to set up there are some drawbacks for this approach, like the need for the image to support incremental mode. Also, in this scenario, as the build process saves the dependencies within the image being built it means that if successive builds are run in different nodes, every node will have to first pull down the image from the OpenShift’s Docker registry which might take longer than pulling down the dependencies again.\n The most important benefit of using Nexus or any other artifact repository dependency manager is the security and the fact that dependencies downloaded by one developer/build will be reused over all the builds using the same dependencies. Whereas in the case of incremental builds only the dependencies downloaded during previous build can be reused and only by the same build. This might have huge impact for any Java-based organization.\n In this blog, I\u0026#8217;ve highlighted how we can improve the build time of Maven based Java builds in OpenShift, but also a very important topic is the use of the internal DNS service names to reference from one project to another. The only caveat to this, is that if we are using the multi-tenant OVS networking plugin, our cluster administrators will have to make visible our ci project to all other projects:\n $ oadm pod-network make-projects-global ci     ","description":"","id":41,"section":"posts","tags":["openshift","origin","builds","maven","java","nexus"],"title":"Improving Build Time of Java Builds on OpenShift","uri":"http://jorgemoral.es/posts/2016-01-18-speed-java-builds/"},{"content":"We live in a polyglot world where developers are using a vast array of different technologies to create applications that perform well, while also having the ability to scale to meet the demands of their application users. Of course, it is very easy to show the supported languages and runtimes that OpenShift provides out of the box, but to be realistic, many developers would like to see how we can bring other leading technologies into OpenShift and use them seamlessly. We built OpenShift 3 around Docker to embrace a standard container runtime and packaging format and give OpenShift developers access to the huge ecosystem of Docker-packaged software stacks. Kubernetes then adds web scale orchestration to OpenShift, which is critical for deploying complex microservices that span multiple containers across multiples hosts. As a result, the ability to run virtually any runtime or framework on the OpenShift platform is now a reality.\n Today, I’ve decided to create an application (to be more precise a set of microservices) using the popular Spring Boot technology and deploy it on OpenShift 3. In this post, I’m going to walk you through all of the steps required, as well as provide the source code so you can see it in action by yourself and customize/extend it to your needs.\n This is a summary of what I’m going to show:\n   Creating a Source-to-Image (S2I) builder image for Spring Boot based applications\n  Deploying a sample set of applications developed with Spring Boot\n   Let’s get our hands dirty and start playing with some of this technology.\n   Spring Boot S2I Builder Spring Boot applications are really very similar in structure to any other Java application. The main difference, apart from the libraries you use, is that it will pack everything as a single fat jar that will be run directly by the JVM. All of the runtime, libraries, dependencies and code will be embedded into this single jar file.\n OpenShift S2I automatically generates a new Docker image for deployment, using source code provided by the developer and a corresponding Docker builder image in OpenShift. I used what Maciej Szulik showed in his How to Create an S2I Builder Image blog post. Also, I used the openshift/wildfly-81-centos7 image as a source of inspiration.\n I created an S2I project, and modified the Dockerfile:\n   Installed all the binaries I needed. In this case I installed java 8, maven 3.3.3 and gradle 2.6 on top of the openshift/base-centos7 base image\n  Modified all the labels describing the S2I image\n   You may want to take a few minutes and examine the Dockerfile in more detail.\n The next step in the process of getting a Spring Boot application running on OpenShift 3 is to build/generate the artifact (fat jar file) and place it into a “known” location where the image expects to find it. In order to accomplish this, we need to modify the assemble script that we are using as a base from the wildfly image. This scripts needs to be modified to have the following capabilities:\n   Ability to build with maven and gradle. Maven takes precedence if there is a pom.xml against having a build.gradle file\n  Make configurable the options to both builders via and ENV named BUILDER_ARGS with some appropriate defaults\n  Place the generated artifact in a known location (/opt/openshift/app.jar)\n   You may want to take a few minutes and review the modified assemble script to ensure you understand all of the changes.\n And finally, I modified the S2I run script to run the Spring Boot application, with the following changes:\n   Execution of the generated artifact in the known location\n  Being able to pass application parameters via an ENV named APP_OPTIONS\n   You may want to look in greater detail the S2I run script.\n Then, following instructions on Maciej\u0026#8217;s blog post, I created a test application (really two, one for maven building and one for gradle), and tested everything via using the generated Makefile.\n Great!!! Now it is time to add this S2I Builder to OpenShift 3.\n At this step you can take two approaches (with some variations). Publish your builder on Docker Hub, or just make your builder available in OpenShift..\n Now, in order to do the latter, in a new project we need to:\n   Create an ImageStream. An ImageStream is identified as having builder images if it has a builder tag in it’s definition.\n  Create a BuildConfig using my GitHub project as source and the builder ImageStream as output.\n   You may want to see the OpenShift resources definition.\n To have the Spring Boot builder working on your OpenShift 3 installation, follow these simple steps:\n $ oc new-project springboot-sti $ oc create -f https://raw.githubusercontent.com/jorgemoralespou/osev3-examples/master/spring-boot/springboot-sti/springboot-sti-all.json   I have also provided two sample instant-app templates for demonstrating usage of the builder. This is a helloworld Spring Boot sample application available in my GitHub that will be built using maven or gradle depending on the instant-app you select.\n You can monitor the building process for this springboot-sti image. Once the build is done, and the image is pushed into the internal docker registry in OpenShift, it is ready for use.\n   You can use one of the quickstart templates:\n   Provide it with required information (application name and hostname):\n   And create the application.\n   Once your application is running, you can visit it’s URL to see it in action.\n     A more interesting Spring Boot application So far, we have a running Spring Boot application. But the question is, will I be able to build and run more complex applications? Of course you can!\n I created a more complex sample Spring Boot application, consisting of a web application and a messages service that stores information in memory.To keep things simple, I used the samples provided with Spring Boot distribution as source and modified two of them to allow interaction across the two samples. So our web application will be a frontend for our messages service. The web service will interact with the messages service via a Rest API.\n I will use the same approach I did before, on Part 2 of my templates blog to show all the OpenShift resources that I will be creating. I will package everything as an instant-app template, so it can be seen in action in an easy way.\n   To run it you will just need to load the instant-app, and instantiate it. Of course, you will need to adjust the parameters when creating your template. The required parameters are APPLICATION_NAME for the name of the application, APPLICATION_HOSTNAME will be the external DNS name where the web will be listening and APPLICATION_HOSTNAME_DATA will be the external DNS name for the Rest Endpoint for the data service.\n $ oc create -f https://raw.githubusercontent.com/jorgemoralespou/osev3-examples/master/spring-boot/sample-microservices-springboot/ose-instantapp-template.json $ oc new-app --template=springboot-sample-microservices -p APPLICATION_NAME=springbootms,APPLICATION_HOSTNAME=web.example.com,APPLICATION_HOSTNAME_DATA=data.example.com   This sample application will create a web component that will look like this:\n   And a data services, that can be queried using Rest, like this:\n $ curl http://data.example.com/ [{\"id\":1,\"text\":\"Hello\",\"summary\":\"World\",\"created\":1441125685591},{\"id\":2,\"text\":\"Hi\",\"summary\":\"Universe\",\"created\":1441125685594},{\"id\":3,\"text\":\"Hola\",\"summary\":\"OpenShift\",\"created\":1441125685594}]   $ curl -H \"Content-type: application/json\" -X POST -d '{\"id\":10,\"text\":\"aaaaa\",\"summary\":\"bbbbb\"}' http://data.example.com:1080 {\"id\":10,\"text\":\"aaaaa\",\"summary\":\"bbbbb\",\"created\":1441126793364}   $ curl http://data.example.com/ [{\"id\":1,\"text\":\"Hello\",\"summary\":\"World\",\"created\":1441125685591},{\"id\":2,\"text\":\"Hi\",\"summary\":\"Universe\",\"created\":1441125685594},{\"id\":3,\"text\":\"Hola\",\"summary\":\"OpenShift\",\"created\":1441125685594},{\"id\":10,\"text\":\"aaaaa\",\"summary\":\"bbbbb\",\"created\":1441126793364}]   Looking at the logs of both pods, you will be able to see the output of your running Spring Boot applications.\n Let’s first identify our pods. These will be the pods in Running state, with names starting with springbootms-data and springbootms-web:\n $ oc get pods NAME READY STATUS RESTARTS AGE springboot-sti-1-build 0/1 ExitCode:0 0 48m springbootms-data-1-1093k 1/1 Running 0 24m springbootms-data-1-build 0/1 ExitCode:0 0 28m springbootms-web-1-37xi2 1/1 Running 0 24m springbootms-web-1-build 0/1 ExitCode:0 0 28m   This is similar to what you will see if you tail the log for the data service:\n $ oc logs springbootms-data-1-1093k 2015-09-01 16:41:28.019 INFO 1 --- [ main] o.s.j.e.a.AnnotationMBeanExporter : Registering beans for JMX exposure on startup 2015-09-01 16:41:28.031 INFO 1 --- [ main] o.s.c.support.DefaultLifecycleProcessor : Starting beans in phase 0 2015-09-01 16:41:28.239 INFO 1 --- [ main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http) 2015-09-01 16:41:28.241 INFO 1 --- [ main] c.o.e.m.r.InMemoryRepositoryApplication : Started InMemoryRepositoryApplication in 19.117 seconds (JVM running for 20.961) 2015-09-01 16:55:36.809 INFO 1 --- [nio-8080-exec-4] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring FrameworkServlet 'dispatcherServlet' 2015-09-01 16:55:36.809 INFO 1 --- [nio-8080-exec-4] o.s.web.servlet.DispatcherServlet : FrameworkServlet 'dispatcherServlet': initialization started 2015-09-01 16:55:36.836 INFO 1 --- [nio-8080-exec-4] o.s.web.servlet.DispatcherServlet : FrameworkServlet 'dispatcherServlet': initialization completed in 27 ms   And this is the content available in the tailed log for the web service:\n $ oc logs springbootms-web-1-37xi2 2015-09-01 16:41:27.410 INFO 1 --- [ main] o.s.j.e.a.AnnotationMBeanExporter : Registering beans for JMX exposure on startup 2015-09-01 16:41:27.693 INFO 1 --- [ main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http) 2015-09-01 16:41:27.703 INFO 1 --- [ main] c.o.e.m.web.SampleWebUIApplication : Started SampleWebUIApplication in 17.639 seconds (JVM running for 20.512) 2015-09-01 16:55:36.567 INFO 1 --- [nio-8080-exec-4] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring FrameworkServlet 'dispatcherServlet' 2015-09-01 16:55:36.568 INFO 1 --- [nio-8080-exec-4] o.s.web.servlet.DispatcherServlet : FrameworkServlet 'dispatcherServlet': initialization started 2015-09-01 16:55:36.594 INFO 1 --- [nio-8080-exec-4] o.s.web.servlet.DispatcherServlet : FrameworkServlet 'dispatcherServlet': initialization completed in 26 ms   As we have seen, our sample Spring Boot services application are running fine using our Spring Boot S2I builder image.\n I hope you have enjoyed!!!\n   ","description":"","id":45,"section":"posts","tags":["openshift","origin","builds","springboot","java","maven","s2i","gradle"],"title":"Using OpenShift for Enterprise Grade Spring Boot Deployments","uri":"http://jorgemoral.es/posts/2015-09-03-deploying-springboot/"},{"content":"This is Part 2 of a 2 part series of blogs that will help you bringing your applications into OpenShift.\n Now that we already know what is a template, and why we should use templates, let\u0026#8217;s walk through the process of creating a template for our application.\n   Our application For this example, we are going to bring into OpenShift an application that will display a map and perform geospatial queries to populate the map with all Major League Baseball stadiums in the United States. Source for this application can be found in my openshift3mlbparks GitHub repository.\n The deployment architecture will consist of:\n   JBoss Enterprise Application Server with a JavaEE application as frontend tier\n  MongoDB server with data corresponding to the location of the MLB Stadiums in the US as backend/data tier\n   As the frontend tier is stateless, we will be able to deploy many JBoss EAP instances.\n We want to access our application in a single DNS name (e.g. mlbsparks.cloudapps.example.com).\n     Design our template The first thing we will need to do is design the contents of our template. The best approach I\u0026#8217;ve found so far is to think of a template as a set of layers of resources with the following structure (from bottom up):\n   OpenShift Images: Base images we will be using for our containers.\n  Builds: Generate an image from source code (application source or Dockerfile source).\n  Images: Images produced by the builds.\n  Deployments: What images will be deployed and how.\n  Abstractions: Additional resources needed for our application, like networking, storage, security,\u0026#8230;\u0026#8203;\n   Layer 0: OpenShift images In this first layer, we will need to define all the \"base\" images we will be using for our containers. These images typically will not be part of the template, but they need to be identified. These can be S2I images or plain Docker images.\n  ImageStream  An image stream presents a single virtual view of related images, as it may contain images from:\n  Its own image repository in OpenShift’s integrated Docker Registry\n  Other image streams\n  Docker image repositories from external registries.\n      :: OpenShift stores complete metadata about each image (e.g., command, entrypoint, environment variables, etc.). Images in OpenShift are immutable.\n  ImageStreamImage  An ImageStreamImage is used to reference or retrieve an image for a given image stream and image name. It uses the following convention for its name: \u0026lt;image stream name\u0026gt;@\u0026lt;name\u0026gt;\n ImageStreamTag  An ImageStreamTag is used to reference or retrieve an image for a given image stream and tag. It uses the following convention for its name: \u0026lt;image stream name\u0026gt;:\u0026lt;tag\u0026gt;\n   In our sample application, we will be using 2 base images:\n   for the frontend component of our application, where we will be using a S2I enabled JBoss EAP image. We will be using a specific tag, 6.4 of this image. As this image will be used for building purposes, the specific usage will be defined in the Build layer.\n  for the backend component of our application we will be using a MongoDB database. We will be using the latest available image. As this image is a ready to use image, the specific usage of this image will be defined in the Deployment layer.\n     Note  Both ImageStreams are provided by OpenShift 3 out of the box, hence they are installed in the openshift project (namespace).     For more information see the official documentation.\n  Layer 1: Builds This layer defines all the builds we will require for our application. A build is the process of transforming input parameters into a resulting object. Most often, the process is used to transform source code into a runnable image.\n  BuildConfig  A BuildConfig object is the definition of the entire build process. :: A build configuration consists of the following key parts:\n  A source description (Where is your source code?)\n  A strategy for building (How to build your image?)\n  Source-To-Image: Transform your application into a runnable docker image, using a S2I image for building and running your application.\n  Docker: Your Dockerfile will be built into an image. This image will contain both, the runtime and the application already built.\n  Custom: You provide the building method in a Docker image.\n     An output description (Where to place the built image?)\n  A list of triggers (When and Why will the source be built?)\n      In our sample application we will be building the frontend component, layering our application on top of an EAP runtime.\n { \"kind\": \"BuildConfig\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"mlbparks\", # (1) \"labels\": { \"application\": \"mlbparks\" # (2) } }, \"spec\": { \"source\": { # (3) \"type\": \"Git\", # (4) \"git\": { \"uri\": \"https://github.com/jorgemoralespou/openshift3mlbparks.git\", # (5) \"ref\": \"master\" # (6) }, \"contextDir\":\"\" # (7) }, \"strategy\": { # (8) \"type\": \"Source\", # (9) \"sourceStrategy\": { \"from\": { # (10) \"kind\": \"ImageStreamTag\", \"namespace\": \"openshift\", \"name\": \"jboss-eap6-openshift:6.4\" } } }, \"output\": { # (11) \"to\": { \"kind\": \"ImageStreamTag\", \"name\": \"mlbparks:latest\" } }, \"triggers\": [ { \"type\": \"GitHub\", # (12) \"generic\": { \"secret\": \"secret\" } }, { \"type\": \"Generic\", # (13) \"github\": { \"secret\": \"secret\" } }, { \"type\": \"ImageChange\", # (14) \"imageChange\": {} } ] } }     This is the name that will identify this BuildConfig.\n  These are the labels that will be set for this BuildConfig.\n  This section defines where is the source for the build.\n  It defines it is source located in a Git repository.\n  In this URI.\n  And using this tag/branch. This value is optional and defaults to “master” if not provided.\n  And this subdirectory from the repository. This value is optional and defaults to the root directory of the repository.\n  This defines which build strategy to use.\n  Source=S2I.\n  And this defines which S2I builder image to use.\n  Defines where to leave the generated image if the build succeeds. It is placing it in our current project.\n  This define that a change generated via a GitHub webhook trigger (if the source code is changed) will trigger a build.\n  This define that a change generated via a Generic webhook trigger will trigger a build.\n  This define that an Image Change will trigger a build. This will trigger a build if the builder image changes or is updated.\n   For more information see the official documentation.\n  Layer 2: Images This layer defines all the images produced by the builds.\n In our sample application we will be producing an image defined in a new ImageStream.\n { \"kind\": \"ImageStream\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"mlbparks\", # (1) \"labels\": { \"application\": \"mlbparks\" # (2) } }, \"spec\": { # (3) \"dockerImageRepository\": \"\", # (4) \"tags\": [ # (5) { \"name\": \"latest\" } ] } }     Name of the ImageStream. This ImageStream will be created in the current project.\n  Label to describe the resource relative to the application we are creating.\n  ImageStream Specifications\n  Docker Repository backing this image stream.\n  List of available tags or image stream locators for this image stream.\n   As a result of the build process, for every build OpenShift will create a new version of the image, that we will always be tagged as latest (as seen in the BuildConfig\u0026#8217;s output spec).\n For more information see the official documentation.\n  Layer 3: Deployments This layer defines the core of our applications. It defines what will be running in OpenShift.\n  DeploymentConfig  A DeploymentConfig is a definition of what will be deployed and running on OpenShift 3.\n   :: A deployment configuration consists of the following key parts:\n   A replication controller template which describes the application to be deployed. (What will be deployed?)\n  The default replica count for the deployment. (How many instances will be deployed and running?)\n  A deployment strategy which will be used to execute the deployment. (How it will be deployed?)\n  A set of triggers which cause deployments to be created automatically. (When and Why will it be deployed?)\n   In our sample application we will have 2 DeploymentConfigs, one for the frontend component (JavaEE application) and another for the backend component (MongoDB).\n The DeploymentConfig for our frontend component will define that:\n   will have a pod with a single container, using the previously built mlbparks image.\n  there will be initially 1 replica\n  there will be a new deployment every time there is a new image built or there is a change in the configuration\n  the redeployment strategy will be \"Recreate\", which means discard all running pods and create new ones.\n   { \"kind\": \"DeploymentConfig\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"mlbparks\", # (1) \"labels\": { # (2) \"deploymentConfig\": \"mlbparks\", \"application\": \"mlbparks\" } }, \"spec\": { # (3) \"replicas\": 1, # (4) \"selector\": { \"deploymentConfig\": \"mlbparks\" # (5) }, \"strategy\": { \"type\": \"Recreate\" # (6) }, \"template\": { # (7) \"metadata\": { \"labels\": { # (8) \"deploymentConfig\": \"mlbparks\", \"application\": \"mlbparks\" }, \"name\": \"mlbparks\" # (9) }, \"spec\": { # (10) \"containers\": [ { \"name\": \"mlbparks\", # (11) \"image\": \"mlbparks\", # (12) \"imagePullPolicy\": \"Always\", # (13) \"env\": [ # (14) { \"name\": \"OPENSHIFT_DNS_PING_SERVICE_NAME\", \"value\": \"mlbparks-ping\" }, { \"name\": \"OPENSHIFT_DNS_PING_SERVICE_PORT\", \"value\": \"8888\" }, { \"name\": \"MONGODB_USER\", \"value\": \"user\" }, { \"name\": \"MONGODB_PASSWORD\", \"value\": \"password\" }, { \"name\": \"MONGODB_DATABASE\", \"value\": \"database\" } ], \"ports\": [ # (15) { \"name\": \"mlbparks-http\", \"containerPort\": 8080, \"protocol\": \"TCP\" }, { \"name\": \"mlbparks-ping\", \"containerPort\": 8888, \"protocol\": \"TCP\" } ], \"readinessProbe\": { # (16) \"exec\": { \"command\": [ \"/bin/bash\", \"-c\", \"/opt/eap/bin/readinessProbe.sh\" ] } }, \"resources\": {}, \"terminationMessagePath\": \"/dev/termination-log\", \"securityContext\": { # (17) \"capabilities\": {}, \"privileged\": false } } ], \"restartPolicy\": \"Always\", \"dnsPolicy\": \"ClusterFirst\" } }, \"triggers\": [ # (18) { \"type\": \"ImageChange\", # (19) \"imageChangeParams\": { \"automatic\": true, \"containerNames\": [ \"mlbparks\" ], \"from\": { \"kind\": \"ImageStreamTag\", \"name\": \"mlbparks:latest\" } } }, { # (20) \"type\": \"ConfigChange\" } ] } }     This is the name that will identify this DeploymentConfig\n  These are the labels that will describe this DeploymentConfig.\n  Specification for the DeploymentConfig. Everything inside this section describes the DeploymentConfig configuration.\n  Number of instances that should be created for this component/deployment\n  This should be the same as name (1).\n  Strategy to use when deploying a new version of the application in case it is triggered. As defined in triggers\n  The template defines what will be deployed as part of this deployment (the pod)\n  The labels to apply for the resources contained in the template (pod)\n  Name of the pod. Every pod instance created will have this name as prefix.\n  Defines the configuration (contents) of the pod\n  The name of the container.\n  The name of the image to use. See note.\n  What should do when deploying. As we will be building the image, we need to always pull on new deployments. Note that if the image tag is latest, it will always pull the image by default, otherwise it will default to “IfNotPresent”.\n  A set of environment variables to pass to this container\n  The ports that the container exposes\n  Probe that will determine if the runtime in the container has started successfully, and traffic can be routed to it.\n  SecurityContextContraint to use for the container\n  The triggers that will dictate on what conditions to create a new deployment. (Deploy a new version of the pod)\n  Create a new deployment when the latest image tag is updated\n  Create a new deployment when there is a configuration change for this Resource.\n     Note  It is always recommended to set in every resource defined by a template a label of type \"application\": \"NAME_OF_MY_APP\" as then you can link resources created as part of the processing of the template. This can be done resource by resource, as described here, or at once, as described later in Labeling all resources in a template.       Note  If there is an ImageChangeTrigger defined for a DeploymentConfig, the image spec\u0026#8217;s value gets substituted with the appropriate value for the image triggering the change. If you don\u0026#8217;t have an ImageChangeTrigger, then this value should be a valid docker pull spec (ie \"openshift/mongodb-24-centos7\").     The DeploymentConfig for our backend component will define that:\n   will have a pod with a single container using the MongoDB openshift base image.\n  there will be initially 1 replica\n  there will be a new deployment every time there is a new image built or there is a change in the configuration\n  the redeployment strategy will be \"Recreate\", which means discard all running pods and create new ones.\n  have a persistent volume on the host\u0026#8217;s filesystem (not valid for HA or host failover).\n   { \"kind\": \"DeploymentConfig\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"mlbparks-MongoDB\", # (1) \"labels\": { # (2) \"application\": \"mlbparks\" } }, \"spec\": { # (3) \"replicas\": 1, # (4) \"selector\": { \"deploymentConfig\": \"mlbparks-MongoDB\" # (5) }, \"strategy\": { \"type\": \"Recreate\" # (6) }, \"template\": { # (7) \"metadata\": { \"labels\": { # (8) \"deploymentConfig\": \"mlbparks-MongoDB\", \"application\": \"mlbparks\" }, \"name\": \"mlbparks-MongoDB\" # (9) }, \"spec\": { # (10) \"containers\": [ { \"name\": \"mlbparks-MongoDB\", # (11) \"image\": \"MongoDB\", # (12) \"imagePullPolicy\": \"IfNotPresent\", # (13) \"env\": [ # (14) { \"name\": \"MONGODB_USER\", \"value\": \"user\" }, { \"name\": \"MONGODB_PASSWORD\", \"value\": \"password\" }, { \"name\": \"MONGODB_DATABASE\", \"value\": \"database\" } ], \"ports\": [ # (15) { \"containerPort\": 27017, \"protocol\": \"TCP\" } ], \"resources\": {}, \"volumeMounts\": [ # (16) { \"name\": \"mlbparks-MongoDB-data\", \"mountPath\": \"/var/lib/MongoDB/data\" } ], \"terminationMessagePath\": \"/dev/termination-log\", \"securityContext\": { # (17) \"capabilities\": {}, \"privileged\": false } } ], \"volumes\": [ # (18) { \"name\": \"mlbparks-MongoDB-data\", \"emptyDir\": {} } ], \"restartPolicy\": \"Always\", \"dnsPolicy\": \"ClusterFirst\" } }, \"triggers\": [ # (19) { \"type\": \"ImageChange\", # (20) \"imageChangeParams\": { \"automatic\": true, \"containerNames\": [ \"mlbparks-MongoDB\" ], \"from\": { \"kind\": \"ImageStreamTag\", \"namespace\": \"openshift\", \"name\": \"MongoDB:latest\" } } }, { # (21) \"type\": \"ConfigChange\" } ] } }     This is the name that will identify this DeploymentConfig\n  These are the labels that will describe this DeploymentConfig.\n  Specification for the DeploymentConfig. Everything inside this section describes the DeploymentConfig configuration.\n  Number of instances that should be created for this component/deployment\n  This should be the same as name (1).\n  Strategy to use when deploying a new version of the application in case it is triggered. As defined in triggers\n  The template defines what will be deployed as part of this deployment (the pod)\n  The labels to apply for the resources contained in the template (pod)\n  Name of the pod. Every pod instance created will have this name as prefix.\n  Defines the configuration (contents) of the pod\n  The name of the container.\n  The name of the image to use. See note.\n  What should do when deploying. We will only pull the image if it is not present, unless there is an ImageChange triggered in which case it will pull down the image, as we are using the :latest tag.\n  A set of environment variables to pass to this container\n  The ports that the container exposes\n  Volume mounts used in the container\n  SecurityContextContraint to use for the container\n  Volumes required for the pod. EmptyDir is a temporary directory on a single machine.\n  The triggers that will dictate on what conditions to create a new deployment. (Deploy a new version of the pod)\n  Create a new deployment when the latest image tag is updated\n  Create a new deployment when there is a configuration change for this Resource.\n   For more information see the official documentation.\n  Layer 4: Abstractions This layer defines all of the additional resources needed for our application to run, like networking, storage, security,\u0026#8230;\u0026#8203;\n  Service  A service serves as an internal load balancer. It identifies a set of replicated pods in order to proxy the connections it receives to them. Backing pods can be added to or removed from a service arbitrarily while the service remains consistently available, enabling anything that depends on the service to refer to it at a consistent internal address. :: Services are assigned an IP address and port pair that, when accessed, proxy to an appropriate backing pod. A service uses a label selector to find all the containers running that provide a certain network service on a certain port.\n Route  An OpenShift route exposes a service at a host name, like www.example.com, so that external clients can reach it by name.\n PersistentVolumeClaim  You can make a request for storage resources using a PersistentVolumeClaim object; the claim is paired with a volume that generally matches your request.\n ServiceAccount  Service accounts provide a flexible way to control API access without sharing a regular user’s credentials.\n Secret  A secret provides a mechanism to hold sensitive information such as passwords, OpenShift client config files, dockercfg files, etc. Secrets decouple sensitive content from the pods that use it and can be mounted into containers using a volume plug-in or used by the system to perform actions on behalf of a pod.\n     Note  These are not all of the possible abstractions. Read the official documentation for more.     In our example, we will need a set of services abstracting the deployments:\n A service for the backend component (MongoDB). This service will be configured to target all pods running created with a label of deploymentConfig=mlbparks-MongoDB which happens for every pod created by the DeploymentConfig specified (as we can see in the DeploymentConfig for the backend component).\n { \"kind\": \"Service\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"MongoDB\", # (1) \"labels\": { \"application\": \"mlbparks\" # (2) } }, \"spec\": { \"ports\": [ { \"port\": 27017, # (3) \"targetPort\": 27017 # (4) } ], \"selector\": { # (5) \"deploymentConfig\": \"mlbparks-MongoDB\" } } }     Name of the service\n  Labels describing this service\n  Port where the service will be listening\n  Port in the pod to route the network traffic to\n  Label selector for determining which pods will be target for this service\n   A service for the frontend component (JBoss EAP). This service will be configured to target all pods running created with a label of deploymentConfig=mlbparks which happens for every pod created by the DeploymentConfig specified (as we can see in the DeploymentConfig for the frontend component).\n { \"kind\": \"Service\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"mlbparks-http\", # (1) \"labels\": { \"application\": \"mlbparks\" # (2) }, \"annotations\": { \"description\": \"The web server's http port\" } }, \"spec\": { \"ports\": [ { \"port\": 8080, # (3) \"targetPort\": 8080 # (4) } ], \"selector\": { \"deploymentConfig\": \"mlbparks\" # (5) } } }     Name of the service\n  Labels describing this service\n  Port where the service will be listening\n  Port in the pod to route the network traffic to\n  Label selector for determining which pods will be target for this service\n   JBoss EAP currently needs an additional service for it\u0026#8217;s internal PING service, that is used for clustering purposes. This service will be configured to target all pods running created with a label of deploymentConfig=mlbparks which happens for every pod created by the DeploymentConfig specified (as we can see in the DeploymentConfig for the frontend component).\n { \"kind\": \"Service\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"mlbparks-ping\", # (1) \"labels\": { \"application\": \"mlbparks\" # (2) }, \"annotations\": { \"description\": \"Ping service for clustered applications\" } }, \"spec\": { \"ports\": [ { \"port\": 8888, # (3) \"targetPort\": 8888 # (4) } ], \"selector\": { \"deploymentConfig\": \"mlbparks\" # (5) } } }     Name of the service\n  Labels describing this service\n  Port where the service will be listening\n  Port in the pod to route the network traffic to\n  Label selector for determining which pods will be target for this service\n   Also, we want our application to be publicly available, so we expose the service providing HTTP access to the frontend component of the application as a route:\n { \"kind\": \"Route\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"mlbparks-http-route\", # (1) \"labels\": { \"application\": \"mlbparks\" # (2) }, \"annotations\": { \"description\": \"Route for application's http service\" } }, \"spec\": { \"host\": \"mlbparks.cloudapps.example.com\", # (3) \"to\": { # (4) \"kind\": \"Service\", \"name\": \"mlbparks-http\" } } }     Name of the route\n  Set of labels to describe the route\n  DNS name used to access our application. This DNS name needs to resolve to the ip address of the OpenShift router.\n  Defines that this is a route to a service with the specified name\n    The result This is a graphical representation of the resources we have created for our application and that will be part of the template:\n      Labeling the template Now, we should have a set of resources that we want to create as part of our \"application\" or \"deployment\" (sometimes how we name it can be confusing). As we want to identify univocally the resources we are deploying as a whole, it is important that all of them have at least one label for this purpose. In the previous code we have set in all of the resources a label of:\n \"application\": \"mlbparks\"   Also, we can set different labels that will help us decorate some other parts of the deployment, like:\n \"deploymentConfig\": \"mlbparks\"   that helps us identify which DeploymentConfig we will link a Service to.\n Labeling all resources in a template A more convenient and concise way of setting labels for the resources in a template is to set the labels to the template resource instead. These labels will be set on every resource created when processing the template.\n { \"kind\": \"Template\", \"apiVersion\": \"v1\", \"metadata\": { ... }, \"labels\": { # (1) \"application\": \"mlbparks\", \"createdBy\": \"template-mlbparks\" }, \"parameters\": [ ... ], \"objects\": [ ... ] }     Labels to describe all resources in the template.\n   In this example we set two labels at the template scope, one that defines that all resources with that label were created by this template, and another label to describe that all resources belongs to the “mlbparks” application. There might be more resources in the future created in the project, that were not initially created by the template, but belongs to the “mlbparks” application.\n  why labels are important Labels can be used for filtering resources on a query, for example:\n $ oc get buildconfig --selector=\"application=mlbparks\" $ oc get deploymentconfig --selector=\"deploymentConfig=mlbparks\"   Also, they can be used to delete in one operation every resource we have created, like:\n $ oc delete all --selector=\"application=mlbparks\"      Make it reusable. Parameterize the template It is time to make the template reusable, as that is the main purpose of a template. For this, we will:\n   Identify what information will be parameterized\n  Change values for parameters placeholders to make the template configurable\n  Create the parameters section for the template\n   After we\u0026#8217;ve done these 3 steps, parameters will be defined and the values will replace the placeholders when creating resources from this template.\n Identify parameters First thing we need to identify is what will be the information in the template we want to parameterize. Here we will be looking into things like the application name, git configuration, secrets, inter component communications configuration, DNS where to expose the Route, \u0026#8230;\u0026#8203;\n  Set the parameter placeholders Once we know the parameters that we will be setting, we will replace the values with a parameter placeholder, so when we process the template, the provided values replace the placeholders.\n A property placeholder will look like:\n ${MY_PARAMETER_NAME}   And we will have something like the following for one of our BuildConfig:\n { \"kind\": \"BuildConfig\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"${APPLICATION_NAME}\", \"labels\": { \"application\": \"${APPLICATION_NAME}\" } }, \"spec\": { \"triggers\": [ { \"type\": \"Generic\", \"generic\": { \"secret\": \"${GENERIC_TRIGGER_SECRET}\" } }, { \"type\": \"GitHub\", \"github\": { \"secret\": \"${GITHUB_TRIGGER_SECRET}\" } }, { \"type\": \"ImageChange\", \"imageChange\": {} } ], \"source\": { \"type\": \"Git\", \"git\": { \"uri\": \"${GIT_URI}\", \"ref\": \"${GIT_REF}\" } }, \"strategy\": { \"type\": \"Source\", \"sourceStrategy\": { \"from\": { \"kind\": \"ImageStreamTag\", \"namespace\": \"openshift\", \"name\": \"jboss-eap6-openshift:${EAP_RELEASE}\" } } }, \"output\": { \"to\": { \"kind\": \"ImageStreamTag\", \"name\": \"${APPLICATION_NAME}:latest\" } } } }    Create the parameters Once we have set all the placeholders in the resources, we will create a section in the template for the parameters. There will be 2 types of parameters:\n   Parameters with auto generated values (using a regexp like expression)\n  Parameters with default values (maybe empty value)\n  Required parameters. When a parameter is required, empty value is not valid (new in OpenShift 3.0.2).\n   \"parameters\": [ { \"description\": \"EAP Release version, e.g. 6.4, etc.\", \"name\": \"EAP_RELEASE\", \"value\": \"6.4\" }, { \"description\": \"The name for the application.\", \"name\": \"APPLICATION_NAME\", \"value\": \"mlbparks\" }, { \"description\": \"Custom hostname for service routes.\", \"name\": \"APPLICATION_HOSTNAME\" }, { \"description\": \"Git source URI for application\", \"name\": \"GIT_URI\", \"value\": \"https://github.com/jorgemoralespou/openshift3mlbparks.git\" }, { \"description\": \"Git branch/tag reference\", \"name\": \"GIT_REF\", \"value\": \"master\" }, { \"description\": \"Database name\", \"name\": \"MONGODB_DATABASE\", \"value\": \"root\" }, { \"description\": \"Database user name\", \"name\": \"MONGODB_USER\", \"from\": \"user[a-zA-Z0-9]{3}\", \"generate\": \"expression\" }, { \"description\": \"Database user password\", \"name\": \"MONGODB_PASSWORD\", \"from\": \"[a-zA-Z0-9]{8}\", \"generate\": \"expression\" }, { \"description\": \"Github trigger secret\", \"name\": \"GITHUB_TRIGGER_SECRET\", \"from\": \"[a-zA-Z0-9]{8}\", \"generate\": \"expression\" }, .... ]     Note  It is important to note that we have generated a random user name and password for the database with an expression and that the values will get injected in the ENV variables for both pods (web and database), so they will be in sync with respect to the user and password credentials to use.     Now we are all set, we do have a template. You can see the full source of the template.\n As can be seen, this template defines 8 new resources.\n    Create the template in OpenShift We need to create the template in OpenShift to make it ready for use. We need to do it with the CLI and we will be able to create it for:\n   General use\n  Only for use in a Project\n   Registering the template for General Use We will execute the creation of the template as user cluster-admin and the template will be registered in the openshift project (which is internal to OpenShift for holding shared resources)\n $ oc create -f my_template.json -n openshift    Registering the template for use in a Project We will execute the creation of the template as a user in the current project. (The user will need to have the appropriate roles to create \"Template\" resources in the current project)\n $ oc create -f my_template.json   If the user belongs to multiple projects, and wants to create the template in a different project from the one he\u0026#8217;s currently working on, he can do it with -n \u0026lt;project\u0026gt;.\n $ oc create -f my_template.json -n \u0026lt;project\u0026gt;      Inspecting a template Before using a template, we need to know:\n   the template name\n  the description of the template\n  the expected parameters\n   List all the available templates For viewing all the available templates for use (using the CLI) we will have to list the templates in the \"openshift\" project and in the user\u0026#8217;s current project.\n $ oc get templates -n openshift $ oc get templates   From this list, we will get the name of the template we want to use.\n  Inspect a template We need more information about the template, so we are going to describe the template:\n $ oc describe template mlbparks Name: mlbparks Created: 7 days ago Labels: \u0026lt;none\u0026gt; Description: Application template for MLB Parks application on EAP 6 \u0026amp; MongoDB built using STI Annotations: iconClass=icon-jboss Parameters: Name: EAP_RELEASE Description: EAP Release version, e.g. 6.4, etc. Value: 6.4 Name: APPLICATION_NAME Description: The name for the application. Value: mlbparks Name: APPLICATION_HOSTNAME Description: Custom hostname for service routes. Value: \u0026lt;none\u0026gt; Name: GIT_URI Description: Git source URI for application Value: https://github.com/jorgemoralespou/openshift3mlbparks.git Name: GIT_REF Description: Git branch/tag reference Value: master Name: MONGODB_DATABASE Description: Database name Value: root Name: MONGODB_NOPREALLOC Description: Disable data file preallocation. Value: \u0026lt;none\u0026gt; Name: MONGODB_SMALLFILES Description: Set MongoDB to use a smaller default data file size. Value: \u0026lt;none\u0026gt; Name: MONGODB_QUIET Description: Runs MongoDB in a quiet mode that attempts to limit the amount of output. Value: \u0026lt;none\u0026gt; Name: MONGODB_USER Description: Database user name Generated: expression From: user[a-zA-Z0-9]{3} Name: MONGODB_PASSWORD Description: Database user password Generated: expression From: [a-zA-Z0-9]{8} Name: MONGODB_ADMIN_PASSWORD Description: Database admin password Generated: expression From: [a-zA-Z0-9]{8} Name: GITHUB_TRIGGER_SECRET Description: Github trigger secret Generated: expression From: [a-zA-Z0-9]{8} Name: GENERIC_TRIGGER_SECRET Description: Generic build trigger secret Generated: expression From: [a-zA-Z0-9]{8} Object Labels: template=mlbparks Objects: BuildConfig ${APPLICATION_NAME} ImageStream ${APPLICATION_NAME} DeploymentConfig ${APPLICATION_NAME}-MongoDB DeploymentConfig ${APPLICATION_NAME} Route ${APPLICATION_NAME}-http-route Service MongoDB Service ${APPLICATION_NAME}-http Service ${APPLICATION_NAME}-ping      Creating resources from a template Now we are ready to instantiate our template. We will provide our own values for the parameters defined in the template. The processing of the template will create all the resources defined by the template in the current project.\n From the Web UI To create the resources from an uploaded template using the web console:\n  While in the desired project, click on the \"Create\u0026#8230;\u0026#8203;\" button:\n      Select a template from the list of templates in your project, or provided by the global template library:\n      View template parameters in the template creation screen:\n      Modify template parameters in the template creation screen by clicking ‘edit parameters’:\n      Click create. This will create all the processed resources defined in the template in the current project. Sequentially, builds and deploys will happen and finally you will have all components ready to accept connections.\n      From the CLI From command line, we can use new-app command to process the template (substitute the parameter placeholders with the values provided) and create the resources in OpenShift.\n We can create the resources using a template that is loaded in OpenShift:\n $ oc new-app mlbparks -p APPLICATION_NAME=mlbparks     Note  We can also specify --template=mlbparks instead of just the template name to be more precise.     Or we can create the resources using the template JSON file:\n $ oc new-app my_template.json -p APPLICATION_NAME=mlbparks     Note  We can also specify --file=my_template.json instead of the template file to be more precise.        Creating a template from existing resources Sometimes it happens that you already have some resources deployed into your project and you want to create a template out of them. OpenShift helps you on this task, and the steps you\u0026#8217;ll need will involve many of the concepts we\u0026#8217;ve already described.\n   Create the template from resources in your project\n  Parameterize the template\n  Deploy the template into OpenShift\n  Instantiate the template (create resources defined in the template with the parameter values supplied by the user)\n   From all these steps, only the first one is new.\n Create a template from a project We can use the existing command oc export to define all the resources in the current project we want to export, and while doing it, we will instruct the command to create a template file, with --as-template=\u0026lt;template_name\u0026gt;.\n $ oc export --as-template=my_template   This will export all the resources in the current project. If we want to limit the resources that should be defined in the template, we can do so:\n // export all services to a template $ oc export service --all --as-template=my_template // export the services and deployment configurations labeled name=test oc export svc,dc -l name=test --as-template=my_template   Remember this will print the template in stdout, so if we want to have the template in a file, we can redirect the output into a file. We can also specify the format for the template as JSON or YAML.\n $ oc export -o json --as-template=my_template \u0026gt; my_template.json $ oc export -o yaml --as-template=my_template \u0026gt; my_template.json   Note: Remember that this export step is really just the beginning of creating a template from existing resources. Once you have the template file, you\u0026#8217;ll have to modify it and adapt it as well as parameterize it to make it configurable.\n    Things you should remember Finally, some important things you should remember when creating templates.\n   Design your template visually, as it helps understand the required components.\n  Provide meaningful names to resources and use labels to describe your resources (labels are used as selectors for some resources).\n  Templates can be shared or per-project, and common templates are in the openshift namespace/project.\n  Currently there is no ability to set a Readme on templates, so be as verbose and complete in the template\u0026#8217;s description.\n  Once the resources in a template are processed and deployed, they can be modified with the CLI.\n  You should constrain the CPU and memory a container in a pod can use.\n  When the resources in a template are created, if there is a BuildConfiguration defined, it will only start an automated build if there is an ImageChange trigger defined. This will change in the next release and we will be able to launch a build on resource creation.\n  Parameterize everything a user of your template might want to customize so they can control the behavior of the template when they instantiate it.\n     ","description":"","id":46,"section":"posts","tags":["openshift","origin","applications","templates"],"title":"Part 2 - Create a template. A technical walkthrough","uri":"http://jorgemoral.es/posts/2015-08-14-creating-templates-2-walkthrough/"},{"content":"This is Part 1 of a 2 part series of blogs that will help you bringing your applications into OpenShift.\n OpenShift 3 allows you to deploy your application to the cloud and the great thing is it doesn’t matter if your cloud is public, private, or even hybrid. Typically, the PaaS platform (OpenShift in this case) will provide a set of predefined runtimes that a developer can use to deploy an application on top of. This developer does not need to worry about the infrastructure, the runtime setup, or the configuration, he/she would just need to focus on their application, and what runtime to use. The PaaS platform will take care of sewing it all together and running it.\n However, sometimes not all the runtimes are provided by the PaaS platform, or that the usages of these runtimes are not suitable for every application type, and there is a need for the PaaS provider to facilitate these to their users.\n As a PaaS provider you’ll need to provide users with:\n   runtimes (a.k.a platforms)\n  configured usages of the runtimes for applications\n     Runtimes As OpenShift 3 relies on containers, the runtimes will be base images that provide the underlying foundation to deploy an application (provided by the user) on top of. The containers also need to be highly configurable so there is no need to provide a single image for every use case. Instead, a different configuration provided to the image will make the runtime work as desired.\n OpenShift 3 provides some base images certified and ready to use out of the box:\n   Node.js\n  Ruby\n  Perl\n  PHP\n  Python\n  JBoss Enterprise Application Platform\n  JBoss A-MQ\n  JBoss Web Server\n   OpenShift provides users with the ability to inject/layer/build source code into these images, as they are created for S2I (Source-To-Image) purposes.\n OpenShift 3 also provides some base images with database runtimes that can be used or extended:\n   MySQL\n  PostgreSQL\n  MongoDB\n     Configured usages of the runtimes OpenShift 3 provides a set of predefined runtime use cases, that are user configurable, and allow for the deployment of applications. These predefined runtimes are modeled as OpenShift templates.\n OpenShift 3 provides the following templates that a Developer can use to simplify the build and deployment process for an application with an existing Git source repository:\n   JavaEE application running on an EAP server\n  JavaEE application running on an EAP server and using an ephemeral database (PostgreSQL, MySQL, MongoDB)\n  JavaEE application running on an EAP server and using a persistent database (PostgreSQL, MySQL, MongoDB)\n  Web application running on a Tomcat Container\n  Web application running on a Tomcat Container and using an ephemeral database (PostgreSQL, MySQL, MongoDB)\n  Web application running on a Tomcat Container and using a persistent database (PostgreSQL, MySQL, MongoDB)\n  ActiveMQ brokers with ephemeral storage\n  ActiveMQ brokers with persistent storage\n  Ephemeral database (PostgreSQL, MySQL, MongoDB)\n  Persistent database (PostgreSQL, MySQL, MongoDB)\n  Instant apps for Perl, Python, Ruby, PHP, Node.js\n     Note  Instant apps are preconfigured example applications including source code that can be forked and altered, providing a quick experience deploying an app in a popular platform.     As you can probably guess, not all possible combinations and capabilities for a runtime or set of runtimes can be provided out of the box, and in many cases, the PaaS provider will have to create more of these for the end user.\n What is a template The official OpenShift 3 documentation states:\n  A template describes a set of objects that can be parameterized and processed to produce a list of objects for creation by OpenShift. The objects to create can include anything that users have permission to create within a project, for example services, build configurations, and deployment configurations. A template may also define a set of labels to apply to every object defined in the template.\n   This means that typically in a template we will have:\n   A set of resources that will be created as part of \"creating/deploying\" the template\n  A set of values for the parameters defined in the template\n  A set of labels to describe the generated resources\n   A template will be defined in JSON or YAML format, and will be loaded into OpenShift for user instantiation, also known as application creation.\n The templates can have global visibility scope (visible for every OpenShift project) or project visibility scope (visible only for a specific project).\n  Benefits of using templates A template provides developers with an easy way to create all the necessary OpenShift resources for their application to work. This allows a developer to quickly deploy an application without having to understand all of the internals of the OpenShift 3 platform.\n   As a PaaS provider you have better control on what is being created and can make better usage of your resources.\n  As a PaaS provider you can define different Service Level Agreements in templates, defining the amount of host resources (cpu, memory) each and every container can consume.\n    Predefined templates, or deploy your application on OpenShift Typically, the PaaS provider will provide users with a set of predefined templates that will cover all of the usages or typologies/topologies of applications that can be deployed on OpenShift.\n The set of predefined templates will be accessible through the CLI or through the Web console.\n When creating your application using one of these templates, the user will typically provide the template with the source for the code of the application and some other configuration items such as the application name, database credentials, etc.\n  Custom templates, or OpenShiftify your application Another use case is when you have a typology/topology of an application that does not fit into the provided templates and you want to create a template to model it. This will be the topic for the next article, a walkthrough on how to create a template for your application.\n    ","description":"","id":47,"section":"posts","tags":["openshift","origin","applications","templates"],"title":"Part 1 - From app to OpenShift","uri":"http://jorgemoral.es/posts/2015-08-13-creating-templates-1/"},{"content":"Some time after we launched, we realized how easy it was to run OpenShift itself as a Docker container, as that’s one of the possible ways to install and run OpenShift. Our lead architect, Clayton Coleman, realized that since every developer will probably have the “oc” (OpenShift client) client tool available on their machines, it could be very easy to add some behaviour to that client to bootstrap a local OpenShift instance. This is how he came with the command cluster and the options up and down.\n “oc cluster up” will start an openshift all-in-one Docker container on your workstation and it will do some bootstrapping to make it usable. With the command comes many switches so that the behaviour can be customized.\n “oc cluster down” will stop that container and remove any configuration used by it.\n A subsequent start of a cluster, again with “oc cluster up” will bring up a fresh new cluster. This means that the cluster that you have started is not persisted by default, unless one uses the switch “--keep-config” that will preserve the configuration upon restarts.\n How does “oc cluster” works? It runs an origin (or ocp) Docker container natively and then it does some bootstrapping to provide some initial configuration. Wait? Did I say a Docker container natively? Yes. This means that for Windows and Mac users you’ll need to use either Docker for Windows or Docker for Mac respectively. Docker for Windows and Docker for Mac uses lightweight virtualization (hyper-V and xhyve respectively) and start a Boot-2-Docker VM, that is very small in size (around 35 MB).\n How to get started Download the oc client from origin releases (lookup for the latest stable)\n   Next, start the cluster:\n oc cluster up     You’ll get a warning about the registry. Just add the registry to the list of insecure registries of your Docker installation.\n   At this point you have a fully functional openshift cluster up and running and available to you. The OpenShift console address is displayed in the output messages, but can also be queried by issuing the following command: at “oc whoami --show-server”. On the startup log there will also be user related information.\n   When you’re done working, just do:\n oc cluster down   There’s a lot of command line switches to customize the cluster behavior. Starting a cluster for a different origin version or configuring proxies for your cluster are some of the things that can be easily configured.\n   Evolution “oc cluster” started as a way for OpenShift engineers to have a cluster up to test their work and soon became the easier way to start a cluster. It was first introduced in 1.3 and it has been adding some features on every release, although just the minimal required features for the intended goal. OpenShift version 1.4/3.4 introduced the ability to bootstrap a proxy and some other behavioral changes. OpenShift 1.5/3.5 will introduce Persistent Volumes, so anytime a cluster is bootstrapped, 100 PV will be created and available for the developer to use. There is a lot of work going into the tool. Although many engineers have contributed to this tool, most of the work has been done by Cesar Wong. So my most sincere kudos to Cesar for his amazing work.\n   Is it enough? Let’s start by saying that I love “oc cluster” as it gives an easy way to bootstrap a cluster based of a Docker container. This is the fastest way to get a cluster up and running. It only requires you to have the oc client, which if you use OpenShift at all you will already have it. And it’s easy to learn. You just need to remember 2 commands “oc cluster up” and “oc cluster down”. On the contrary, I have to say that the default behavior does not make developing applications for that local OpenShift environment agile, as you’ll most likely not use the default behavior and will need to always provide command line switches.\n The goal of this tool is not to provide a local, reusable, development environment, for those that develop applications that will run on OpenShift. It just provides a fast way to have a cluster available. In many cases, this will be sufficient, but not for me, and what I’m looking for as developer of applications for OpenShift.\n The workflow I look for as a developer of applications for Openshift should look like:\n   Start an env\n  Work on it\n  Stop an env\n  Start another env\n  Bootstrap it differently for that project\n  Stop that env\n  Start, develop, stop\n  Destroy a no longer needed env.\n   This has been the main motivation for me to start a side project, a bash script that wraps “oc cluster” and gives me the workflow I’m looking for. The script is named “oc-cluster” and is available on GitHub.\n   “oc-cluster” wrapper: Gaining experience with developers. In the process of creating this tool I have done many experimentation on what would be the common bootstrapping that I would require so I have provided a mechanism to configure anything else that I don’t consider basic in an easy way than what is provided out of the box. There is a plugin mechanism that allows all these add ons to be installed on demand and they can be easily shared between different people.\n The mechanics are the same as the base “oc cluster”, but it provides a default additional parameter which is the name of the cluster to start. This allows the developer to have multiple clusters created and start/stop the one with the work/add ons they want.\n oc-cluster up [PROFILE]     oc-cluster stop     This is all you need to know, but as I have introduced the concept of profiles, you can then list the available clusters to decide which one you want to start in case you don’t remember the name.\n   Also, as the clusters are now long lived, you will be able to completely delete the cluster if you’re not going to work with it any more.\n   The tool has a lot of features targeting make developing on openshift easy, so if you want more information in how this tool works and all the capabilities it has, I recommend you to read the README.adoc in GitHub.\n   Conclusions “oc cluster” it’s an awesome tool to get a cluster up and running, but it really don’t fulfill all my expectations for using it as my local development environment for a day to day development tool. That is the reason has has driven me to create a tool on top.\n The biggest advantage of this tool that created on top is that it totally adjusts to my workflow and expectations, as it is developed by me ;-), and it’s developed in my free time. Windows is not supported as bash does not run there natively. There’s an alternative to this script, written by Graham Dumpleton, written in Python, which supports Windows as well as MacOS X and Linux, called Powershift.\n This side project has been mainly developed to make my daily life easier, but by sharing it, I’ve been collecting a big understanding on what users would expect when working with OpenShift locally, either for development or for any other purpose, like demos or even evangelism. All this feedback is being constantly shared with the people working on “oc cluster” and “minishift”, to make continuously improve these tools, as these are officially provided by Red Hat.\n What’s minishift? minishift is the definitive tool for local OpenShift for development. If you want to know more, don’t forget to read the final blog in this series.\n   ","description":"","id":48,"section":"posts","tags":["openshift","origin","development","local","devexp","oc-cluster"],"title":"Developing locally with OpenShift - “oc cluster up”, the fastest way to get a local cluster","uri":"http://jorgemoral.es/posts/2017-04-07-developing-locally-openshift-oc-cluster/"}]